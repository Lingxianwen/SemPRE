import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Optional, Set
from torch.utils.data import DataLoader, Dataset
import random
from sklearn.metrics import f1_score, accuracy_score, classification_report
from sklearn.preprocessing import LabelEncoder
import os
import matplotlib.pyplot as plt
import seaborn as sns
from collections import defaultdict, Counter
import json
import yaml
from dataclasses import dataclass, field
from pathlib import Path
import argparse
from sklearn.metrics import f1_score, accuracy_score, classification_report


# ===========================
# é…ç½®ç±»ï¼šå®šä¹‰åè®®é…ç½®
# ===========================

@dataclass
class ProtocolConfig:
    """åè®®é…ç½®ç±»ï¼Œå®šä¹‰å•ä¸ªåè®®çš„ç»“æ„"""
    name: str  # åè®®åç§°
    txt_file: str  # åŸå§‹æ•°æ®æ–‡ä»¶è·¯å¾„
    csv_file: str  # æ ‡ç­¾æ–‡ä»¶è·¯å¾„
    parser_func: str  # è§£æå‡½æ•°åç§°
    min_length: int = 8  # æœ€å°æ•°æ®åŒ…é•¿åº¦
    header_patterns: List[bytes] = field(default_factory=list)  # åè®®å¤´éƒ¨ç‰¹å¾
    port: Optional[int] = None  # åè®®ç«¯å£å·
    description: str = ""  # åè®®æè¿°


@dataclass
class TransferConfig:
    """è¿ç§»å­¦ä¹ é…ç½®ç±»"""
    source_protocols: List[str]  # æºåè®®åˆ—è¡¨
    target_protocol: str  # ç›®æ ‡åè®®
    model_params: Dict = field(default_factory=dict)  # æ¨¡å‹å‚æ•°
    training_params: Dict = field(default_factory=dict)  # è®­ç»ƒå‚æ•°
    data_split: Dict = field(default_factory=dict)  # æ•°æ®åˆ†å‰²æ¯”ä¾‹


# ===========================
# æ‰©å±•çš„åè®®æ•°æ®åŠ è½½å™¨ - ä¿®å¤ç‰ˆæœ¬
# ===========================

class AdvancedProtocolDataLoader:
    """é«˜çº§åè®®æ•°æ®åŠ è½½å™¨ï¼Œæ”¯æŒæ›´å¤šçœŸå®åè®®"""

    def __init__(self, data_root: str = "../Msg2", config_file: Optional[str] = None):
        self.data_root = Path(data_root)
        self.protocols: Dict[str, ProtocolConfig] = {}
        self.parsers: Dict[str, callable] = {}

        # æ³¨å†Œå†…ç½®è§£æå™¨
        self._register_builtin_parsers()

        # è‡ªåŠ¨å‘ç°åè®®æ•°æ®
        self._auto_discover_protocols()

        # å¦‚æœæä¾›äº†é…ç½®æ–‡ä»¶ï¼Œåˆ™åŠ è½½
        if config_file and os.path.exists(config_file):
            self.load_config(config_file)

        # ç»Ÿä¸€è¯­ä¹‰æ ‡ç­¾ä½“ç³»
        self.unified_semantic_types = [
            'PADDING', 'HEADER', 'ADDRESS', 'COMMAND', 'LENGTH',
            'DATA', 'CHECKSUM', 'CONTROL', 'FUNCTION', 'OPTION',
            'TIMESTAMP', 'VERSION', 'FLAGS', 'PAYLOAD'
        ]

        self.unified_semantic_functions = [
            'UNKNOWN', 'IDENTIFIER', 'ADDRESSING', 'CONTROL_CMD',
            'DATA_LENGTH', 'PAYLOAD', 'VALIDATION', 'RESERVED',
            'PROTOCOL_SPECIFIC', 'CONFIGURATION', 'SESSION_MGMT',
            'SECURITY', 'ROUTING', 'APPLICATION_DATA'
        ]

    def _auto_discover_protocols(self):
        """è‡ªåŠ¨å‘ç°å¯ç”¨åè®®æ•°æ®"""
        print("ğŸ” è‡ªåŠ¨å‘ç°åè®®æ•°æ®...")

        # å®šä¹‰æ”¯æŒçš„åè®®åŠå…¶é»˜è®¤é…ç½®
        supported_protocols = {
            'smb': {
                'parser_func': 'parse_smb',
                'min_length': 32,
                'port': 445,
                'description': 'Server Message Blockåè®®'
            },
            'smb2': {
                'parser_func': 'parse_smb2',
                'min_length': 64,
                'port': 445,
                'description': 'Server Message Block v2åè®®'
            },
            'dns': {
                'parser_func': 'parse_dns',
                'min_length': 12,
                'port': 53,
                'description': 'Domain Name Systemåè®®'
            },
            's7comm': {
                'parser_func': 'parse_s7comm',
                'min_length': 8,
                'port': 102,
                'description': 'Siemens S7COMMåè®®'
            },
            'dnp3': {
                'parser_func': 'parse_dnp3',
                'min_length': 10,
                'port': 20000,
                'description': 'Distributed Network Protocol 3'
            },
            'modbus': {
                'parser_func': 'parse_modbus',
                'min_length': 8,
                'port': 502,
                'description': 'Modbus TCPåè®®'
            },
            'ftp': {
                'parser_func': 'parse_ftp',
                'min_length': 4,
                'port': 21,
                'description': 'File Transfer Protocol'
            },
            'tls': {
                'parser_func': 'parse_tls',
                'min_length': 5,
                'port': 443,
                'description': 'Transport Layer Security 1.2'
            },
            'dhcp': {
                'parser_func': 'parse_dhcp',
                'min_length': 240,
                'port': 67,
                'description': 'Dynamic Host Configuration Protocol'
            }
        }

        discovered_protocols = []

        for protocol_name, default_config in supported_protocols.items():
            # æŸ¥æ‰¾TXTå’ŒCSVæ–‡ä»¶
            txt_candidates = list(self.data_root.glob(f"txt/**/*{protocol_name}*.txt"))
            csv_candidates = list(self.data_root.glob(f"csv/**/*{protocol_name}*.csv"))

            # ä¹Ÿå°è¯•å¤§å†™å½¢å¼
            txt_candidates.extend(self.data_root.glob(f"txt/**/*{protocol_name.upper()}*.txt"))
            csv_candidates.extend(self.data_root.glob(f"csv/**/*{protocol_name.upper()}*.csv"))

            if txt_candidates and csv_candidates:
                txt_file = str(txt_candidates[0])
                csv_file = str(csv_candidates[0])

                protocol_config = ProtocolConfig(
                    name=protocol_name,
                    txt_file=txt_file,
                    csv_file=csv_file,
                    **default_config
                )

                self.protocols[protocol_name] = protocol_config
                discovered_protocols.append(protocol_name)
                print(f" å‘ç°åè®®: {protocol_name}")
                print(f"     - TXT: {txt_file}")
                print(f"     - CSV: {csv_file}")

        if not discovered_protocols:
            print(" æœªå‘ç°åè®®æ•°æ®æ–‡ä»¶")
        else:
            print(f" æ€»å…±å‘ç° {len(discovered_protocols)} ä¸ªåè®®: {discovered_protocols}")

    def _register_builtin_parsers(self):
        """æ³¨å†Œå†…ç½®åè®®è§£æå™¨"""
        self.parsers.update({
            'parse_modbus': self._parse_modbus,
            'parse_dnp3': self._parse_dnp3,
            'parse_s7comm': self._parse_s7comm,
            'parse_smb': self._parse_smb,
            'parse_smb2': self._parse_smb2,
            'parse_dns': self._parse_dns,
            'parse_ftp': self._parse_ftp,
            'parse_tls': self._parse_tls,
            'parse_dhcp': self._parse_dhcp,
            'parse_generic': self._parse_generic
        })

    def get_available_protocols(self) -> List[str]:
        """è·å–å¯ç”¨åè®®åˆ—è¡¨"""
        return list(self.protocols.keys())

    def register_protocol(self, protocol_config: ProtocolConfig, parser_func: callable = None):
        """åŠ¨æ€æ³¨å†Œæ–°åè®®"""
        self.protocols[protocol_config.name] = protocol_config

        if parser_func:
            self.parsers[protocol_config.parser_func] = parser_func

        print(f"åè®® '{protocol_config.name}' æ³¨å†ŒæˆåŠŸ")

    def load_protocol_data(self, protocol_name: str) -> List[Dict]:
        """åŠ è½½æŒ‡å®šåè®®çš„æ•°æ® - ä¿®å¤ç‰ˆæœ¬"""
        if protocol_name not in self.protocols:
            raise ValueError(f"æœªçŸ¥åè®®: {protocol_name}. å¯ç”¨åè®®: {self.get_available_protocols()}")

        config = self.protocols[protocol_name]
        parser_func = self.parsers.get(config.parser_func, self._parse_generic)

        print(f"åŠ è½½ {protocol_name.upper()} åè®®æ•°æ®...")
        print(f"  - æ•°æ®æ–‡ä»¶: {config.txt_file}")
        print(f"  - æ ‡ç­¾æ–‡ä»¶: {config.csv_file}")

        return parser_func(config)

    # ========== åè®®è§£æå™¨å®ç° - ä¿®å¤ç‰ˆæœ¬ ==========

    def _parse_modbus(self, config: ProtocolConfig) -> List[Dict]:
        """Modbusåè®®è§£æå™¨"""
        return self._parse_with_new_labels(config, self._create_modbus_ground_truth)

    def _parse_dnp3(self, config: ProtocolConfig) -> List[Dict]:
        """DNP3åè®®è§£æå™¨"""
        return self._parse_with_new_labels(config, self._create_dnp3_ground_truth)

    def _parse_s7comm(self, config: ProtocolConfig) -> List[Dict]:
        """S7COMMåè®®è§£æå™¨"""
        return self._parse_with_new_labels(config, self._create_s7comm_ground_truth)

    def _parse_smb(self, config: ProtocolConfig) -> List[Dict]:
        """SMBåè®®è§£æå™¨"""
        return self._parse_with_new_labels(config, self._create_smb_ground_truth)

    def _parse_smb2(self, config: ProtocolConfig) -> List[Dict]:
        """SMB2åè®®è§£æå™¨"""
        return self._parse_with_new_labels(config, self._create_smb2_ground_truth)

    def _parse_dns(self, config: ProtocolConfig) -> List[Dict]:
        """DNSåè®®è§£æå™¨"""
        return self._parse_with_new_labels(config, self._create_dns_ground_truth)

    def _parse_ftp(self, config: ProtocolConfig) -> List[Dict]:
        """FTPåè®®è§£æå™¨"""
        return self._parse_with_new_labels(config, self._create_ftp_ground_truth)

    def _parse_tls(self, config: ProtocolConfig) -> List[Dict]:
        """TLS1.2åè®®è§£æå™¨"""
        return self._parse_with_new_labels(config, self._create_tls_ground_truth)

    def _parse_dhcp(self, config: ProtocolConfig) -> List[Dict]:
        """DHCPåè®®è§£æå™¨"""
        return self._parse_with_new_labels(config, self._create_dhcp_ground_truth)

    def _parse_with_new_labels(self, config: ProtocolConfig, ground_truth_func) -> List[Dict]:
        """ä½¿ç”¨æ–°çš„æ ‡ç­¾æ ¼å¼è§£æåè®®æ•°æ® - ä¿®å¤ç‰ˆæœ¬"""
        try:
            # è¯»å–HEXæ•°æ®
            with open(config.txt_file, 'r', encoding='utf-8') as f:
                hex_packets = [line.strip() for line in f if line.strip()]

            # è¯»å–æ ‡ç­¾æ•°æ®
            df = pd.read_csv(config.csv_file)

            # æ£€æŸ¥å¿…è¦çš„åˆ—
            required_columns = ['HexData', 'FunctionCode', 'Boundaries', 'SemanticTypes', 'SemanticFunctions']
            missing_columns = [col for col in required_columns if col not in df.columns]

            if missing_columns:
                print(f"è­¦å‘Š: CSVæ–‡ä»¶ç¼ºå°‘åˆ—: {missing_columns}")
                # ä½¿ç”¨æ—§çš„è§£ææ–¹å¼
                return self._parse_with_labels(config, ground_truth_func)

            min_len = min(len(hex_packets), len(df))
            data = []
            valid_boundary_samples = 0

            for i in range(min_len):
                try:
                    # ä»DataFrameè·å–æ•°æ®
                    row = df.iloc[i]
                    hex_data = row['HexData']
                    function_code = row['FunctionCode']

                    # æ¸…ç†hexæ•°æ®
                    hex_data = hex_data.replace(' ', '').replace('\t', '')
                    if len(hex_data) % 2 != 0:
                        continue

                    raw_bytes = bytes.fromhex(hex_data)
                    if len(raw_bytes) < config.min_length:
                        continue

                    # ã€ä¿®å¤ã€‘è§£æè¾¹ç•Œä¿¡æ¯ - æ”¹è¿›è¾¹ç•Œå¤„ç†é€»è¾‘
                    boundaries_str = row.get('Boundaries', '')
                    boundaries = []
                    if boundaries_str and str(boundaries_str) != 'nan' and boundaries_str != '':
                        try:
                            # å¤„ç†é€—å·åˆ†éš”çš„è¾¹ç•Œ
                            boundary_parts = str(boundaries_str).split(',')
                            for part in boundary_parts:
                                part = part.strip()
                                if part and part != '-1':  # è¿‡æ»¤æ‰-1å’Œç©ºå€¼
                                    boundaries.append(int(part))
                        except ValueError as e:
                            print(f"è¾¹ç•Œè§£æé”™è¯¯(è¡Œ{i}): {boundaries_str} - {e}")
                            continue

                    # ã€ä¿®å¤ã€‘è¿‡æ»¤æ— æ•ˆè¾¹ç•Œå¹¶æ’åº
                    boundaries = [b for b in boundaries if 0 <= b < len(raw_bytes)]
                    boundaries = sorted(list(set(boundaries)))  # å»é‡å¹¶æ’åº

                    # ã€æ–°å¢ã€‘éªŒè¯è¾¹ç•Œè´¨é‡
                    if len(boundaries) == 0:
                        # å¦‚æœæ²¡æœ‰è¾¹ç•Œï¼Œä½¿ç”¨åŸºäºåè®®çš„é»˜è®¤è¾¹ç•Œ
                        boundaries = self._generate_default_boundaries(raw_bytes, config.name)

                    # è§£æè¯­ä¹‰ä¿¡æ¯
                    semantic_types = {}
                    semantic_functions = {}

                    try:
                        if 'SemanticTypes' in row and pd.notna(row['SemanticTypes']):
                            semantic_types = json.loads(row['SemanticTypes'])
                    except:
                        pass

                    try:
                        if 'SemanticFunctions' in row and pd.notna(row['SemanticFunctions']):
                            semantic_functions = json.loads(row['SemanticFunctions'])
                    except:
                        pass

                    # åˆ›å»ºground truth
                    ground_truth = {
                        'syntax_boundaries': boundaries,
                        'semantic_types': {str(k): v for k, v in semantic_types.items()},
                        'semantic_functions': {str(k): v for k, v in semantic_functions.items()}
                    }

                    sample = {
                        'raw_data': hex_data,
                        'protocol': config.name,
                        'function_code': function_code,
                        'ground_truth': ground_truth,
                        'length': len(raw_bytes)
                    }
                    data.append(sample)

                    if len(boundaries) > 0:
                        valid_boundary_samples += 1

                except Exception as e:
                    print(f"å¤„ç†ç¬¬ {i} è¡Œæ•°æ®æ—¶å‡ºé”™: {e}")
                    continue

            print(f"æˆåŠŸåŠ è½½ {len(data)} æ¡ {config.name.upper()} æ•°æ®")
            print(f"  - æœ‰æ•ˆè¾¹ç•Œæ ·æœ¬: {valid_boundary_samples}/{len(data)}")
            return data

        except Exception as e:
            print(f"è§£æ {config.name} å¤±è´¥: {e}")
            # å›é€€åˆ°æ—§çš„è§£ææ–¹å¼
            return self._parse_with_labels(config, ground_truth_func)

    def _generate_default_boundaries(self, raw_bytes: bytes, protocol_name: str) -> List[int]:
        """ç”Ÿæˆé»˜è®¤è¾¹ç•Œ - åŸºäºåè®®ç‰¹å¾"""
        boundaries = []

        if protocol_name == 'modbus':
            # Modbus TCPç»“æ„: MBAPå¤´(7å­—èŠ‚) + PDU
            if len(raw_bytes) >= 8:
                boundaries = [0, 2, 4, 6, 7]  # ä¸»è¦å­—æ®µè¾¹ç•Œ
                if len(raw_bytes) > 8:
                    boundaries.append(8)  # æ•°æ®å¼€å§‹

        elif protocol_name == 'dnp3':
            # DNP3ç»“æ„
            if len(raw_bytes) >= 10:
                boundaries = [0, 2, 3, 4, 6, 8, 10]  # åŸºäºDNP3ç»“æ„

        else:
            # é€šç”¨åè®®ï¼šæŒ‰å›ºå®šé—´éš”åˆ’åˆ†
            step = max(2, len(raw_bytes) // 8)  # æœ€å°‘8ä¸ªæ®µ
            boundaries = list(range(0, len(raw_bytes), step))

        # ç¡®ä¿è¾¹ç•Œåœ¨æœ‰æ•ˆèŒƒå›´å†…
        boundaries = [b for b in boundaries if 0 <= b < len(raw_bytes)]
        return sorted(boundaries)

    def _parse_with_labels(self, config: ProtocolConfig, ground_truth_func) -> List[Dict]:
        """é€šç”¨çš„å¸¦æ ‡ç­¾è§£ææ–¹æ³• - ä¿ç•™ä½œä¸ºå¤‡ç”¨"""
        try:
            # è¯»å–HEXæ•°æ®
            with open(config.txt_file, 'r', encoding='utf-8') as f:
                hex_packets = [line.strip() for line in f if line.strip()]

            # è¯»å–æ ‡ç­¾æ•°æ®
            df = pd.read_csv(config.csv_file)

            # ç¡®å®šæ ‡ç­¾åˆ—
            label_column = None
            for col in ['Label', 'label', 'Function', 'Type', 'Category', 'FunctionCode']:
                if col in df.columns:
                    label_column = col
                    break

            if label_column is None:
                label_column = df.columns[-1]  # ä½¿ç”¨æœ€åä¸€åˆ—ä½œä¸ºæ ‡ç­¾

            min_len = min(len(hex_packets), len(df))
            data = []

            for i in range(min_len):
                try:
                    hex_data = hex_packets[i]
                    label = df.iloc[i][label_column] if i < len(df) else 'UNKNOWN'

                    # æ¸…ç†hexæ•°æ®
                    hex_data = hex_data.replace(' ', '').replace('\t', '')
                    if len(hex_data) % 2 != 0:
                        continue

                    raw_bytes = bytes.fromhex(hex_data)
                    if len(raw_bytes) >= config.min_length:
                        sample = {
                            'raw_data': hex_data,
                            'protocol': config.name,
                            'function_code': label,
                            'ground_truth': ground_truth_func(raw_bytes, label),
                            'length': len(raw_bytes)
                        }
                        data.append(sample)
                except Exception as e:
                    continue

            print(f"æˆåŠŸåŠ è½½ {len(data)} æ¡ {config.name.upper()} æ•°æ®")
            return data

        except Exception as e:
            print(f"è§£æ {config.name} å¤±è´¥: {e}")
            return []

    def _parse_generic(self, config: ProtocolConfig) -> List[Dict]:
        """é€šç”¨åè®®è§£æå™¨"""
        return self._parse_with_new_labels(config, self._create_generic_ground_truth)

    # ========== Ground Truth åˆ›å»ºå‡½æ•° - ä¿®å¤ç‰ˆæœ¬ ==========

    def _create_modbus_ground_truth(self, raw_bytes: bytes, label) -> Dict:
        """åˆ›å»ºModbus ground truth - ä¿®å¤è¾¹ç•Œé€»è¾‘"""
        annotations = {'syntax_boundaries': [], 'semantic_types': {}, 'semantic_functions': {}}

        if len(raw_bytes) < 8:
            return annotations

        # ã€ä¿®å¤ã€‘Modbus TCPç»“æ„è¾¹ç•Œå®šä¹‰
        # MBAPå¤´: [äº‹åŠ¡ID(2) | åè®®ID(2) | é•¿åº¦(2) | å•å…ƒID(1) | åŠŸèƒ½ç (1) | æ•°æ®(...)]
        boundaries = [0, 2, 4, 6, 7]  # å­—æ®µèµ·å§‹ä½ç½®

        if len(raw_bytes) > 8:
            boundaries.append(8)  # æ•°æ®éƒ¨åˆ†å¼€å§‹

        # è¯­ä¹‰æ ‡æ³¨
        field_mapping = [
            (0, 1, 'HEADER', 'IDENTIFIER'),  # äº‹åŠ¡ID
            (2, 3, 'HEADER', 'PROTOCOL_SPECIFIC'),  # åè®®ID
            (4, 5, 'LENGTH', 'DATA_LENGTH'),  # é•¿åº¦
            (6, 6, 'ADDRESS', 'ADDRESSING'),  # å•å…ƒID
            (7, 7, 'COMMAND', 'CONTROL_CMD'),  # åŠŸèƒ½ç 
        ]

        for start, end, sem_type, sem_func in field_mapping:
            for pos in range(start, min(end + 1, len(raw_bytes))):
                annotations['semantic_types'][str(pos)] = sem_type
                annotations['semantic_functions'][str(pos)] = sem_func

        # æ•°æ®å­—æ®µ
        for i in range(8, len(raw_bytes)):
            annotations['semantic_types'][str(i)] = 'DATA'
            annotations['semantic_functions'][str(i)] = 'PAYLOAD'

        # ç¡®ä¿è¾¹ç•Œåœ¨æœ‰æ•ˆèŒƒå›´å†…
        boundaries = [b for b in boundaries if b < len(raw_bytes)]
        annotations['syntax_boundaries'] = sorted(boundaries)

        return annotations

    def _create_dnp3_ground_truth(self, raw_bytes: bytes, label) -> Dict:
        """åˆ›å»ºDNP3 ground truth - ä¿®å¤è¾¹ç•Œé€»è¾‘"""
        annotations = {'syntax_boundaries': [], 'semantic_types': {}, 'semantic_functions': {}}

        if len(raw_bytes) < 10:
            return annotations

        # ã€ä¿®å¤ã€‘DNP3æ•°æ®é“¾è·¯å±‚ç»“æ„è¾¹ç•Œ
        boundaries = []

        # DNP3æ•°æ®é“¾è·¯å±‚: [èµ·å§‹(2) | é•¿åº¦(1) | æ§åˆ¶(1) | ç›®æ ‡(2) | æº(2) | CRC(2) | ...]
        if len(raw_bytes) >= 2 and raw_bytes[0] == 0x05 and raw_bytes[1] == 0x64:
            boundaries = [0, 2, 3, 4, 6, 8]  # ä¸»è¦å­—æ®µè¾¹ç•Œ
            if len(raw_bytes) > 10:
                boundaries.append(10)  # åº”ç”¨å±‚å¼€å§‹

            # è¯­ä¹‰æ ‡æ³¨
            field_mapping = [
                (0, 1, 'HEADER', 'IDENTIFIER'),  # èµ·å§‹å­—èŠ‚
                (2, 2, 'LENGTH', 'DATA_LENGTH'),  # é•¿åº¦
                (3, 3, 'CONTROL', 'CONTROL_CMD'),  # æ§åˆ¶
                (4, 5, 'ADDRESS', 'ADDRESSING'),  # ç›®æ ‡åœ°å€
                (6, 7, 'ADDRESS', 'ADDRESSING'),  # æºåœ°å€
                (8, 9, 'CHECKSUM', 'VALIDATION'),  # CRC
            ]
        else:
            # åº”ç”¨å±‚æ ¼å¼
            boundaries = [0, 1, 2]
            if len(raw_bytes) > 4:
                boundaries.extend([4, 6, 8])

            field_mapping = [
                (0, 0, 'CONTROL', 'CONTROL_CMD'),  # åº”ç”¨æ§åˆ¶
                (1, 1, 'COMMAND', 'CONTROL_CMD'),  # åŠŸèƒ½ç 
            ]

        # åº”ç”¨è¯­ä¹‰æ ‡æ³¨
        for start, end, sem_type, sem_func in field_mapping:
            for pos in range(start, min(end + 1, len(raw_bytes))):
                annotations['semantic_types'][str(pos)] = sem_type
                annotations['semantic_functions'][str(pos)] = sem_func

        # å‰©ä½™æ•°æ®
        data_start = max(boundaries) + 1 if boundaries else 0
        for i in range(data_start, len(raw_bytes)):
            annotations['semantic_types'][str(i)] = 'DATA'
            annotations['semantic_functions'][str(i)] = 'PAYLOAD'

        # ç¡®ä¿è¾¹ç•Œåœ¨æœ‰æ•ˆèŒƒå›´å†…
        boundaries = [b for b in boundaries if b < len(raw_bytes)]
        annotations['syntax_boundaries'] = sorted(boundaries)

        return annotations

    def _create_s7comm_ground_truth(self, raw_bytes: bytes, label) -> Dict:
        """åˆ›å»ºS7COMM ground truth - ä¿®å¤è¾¹ç•Œé€»è¾‘"""
        annotations = {'syntax_boundaries': [], 'semantic_types': {}, 'semantic_functions': {}}

        if len(raw_bytes) < 8:
            return annotations

        # S7COMMç»“æ„: [TPKTå¤´(4) | COTPå¤´(3) | S7å¤´(...)]
        boundaries = [0, 2, 4, 7]  # TPKTå’ŒCOTPè¾¹ç•Œ

        if len(raw_bytes) > 8:
            boundaries.append(8)  # S7å¤´å¼€å§‹
        if len(raw_bytes) > 12:
            boundaries.append(12)  # S7æ•°æ®å¼€å§‹

        field_mapping = [
            (0, 1, 'HEADER', 'VERSION'),  # TPKTç‰ˆæœ¬+ä¿ç•™
            (2, 3, 'LENGTH', 'DATA_LENGTH'),  # TPKTé•¿åº¦
            (4, 6, 'HEADER', 'SESSION_MGMT'),  # COTPå¤´
            (7, 7, 'HEADER', 'IDENTIFIER'),  # S7åè®®æ ‡è¯†
        ]

        # åº”ç”¨è¯­ä¹‰æ ‡æ³¨
        for start, end, sem_type, sem_func in field_mapping:
            for pos in range(start, min(end + 1, len(raw_bytes))):
                annotations['semantic_types'][str(pos)] = sem_type
                annotations['semantic_functions'][str(pos)] = sem_func

        # S7ç‰¹å®šéƒ¨åˆ†
        for i in range(8, min(12, len(raw_bytes))):
            annotations['semantic_types'][str(i)] = 'COMMAND'
            annotations['semantic_functions'][str(i)] = 'CONTROL_CMD'

        # æ•°æ®éƒ¨åˆ†
        for i in range(12, len(raw_bytes)):
            annotations['semantic_types'][str(i)] = 'DATA'
            annotations['semantic_functions'][str(i)] = 'APPLICATION_DATA'

        boundaries = [b for b in boundaries if b < len(raw_bytes)]
        annotations['syntax_boundaries'] = sorted(boundaries)
        return annotations

    def _create_smb_ground_truth(self, raw_bytes: bytes, label) -> Dict:
        """åˆ›å»ºSMB ground truth"""
        annotations = {'syntax_boundaries': [], 'semantic_types': {}, 'semantic_functions': {}}

        if len(raw_bytes) < 32:
            return annotations

        # SMBå¤´éƒ¨ç»“æ„è¾¹ç•Œ
        boundaries = [0, 4, 5, 9, 25]
        if len(raw_bytes) > 32:
            boundaries.append(32)

        field_mapping = [
            (0, 3, 'HEADER', 'IDENTIFIER'),  # åè®®æ ‡è¯†
            (4, 4, 'COMMAND', 'CONTROL_CMD'),  # SMBå‘½ä»¤
            (5, 8, 'FLAGS', 'CONTROL_CMD'),  # çŠ¶æ€/æ ‡å¿—
            (9, 24, 'HEADER', 'SESSION_MGMT'),  # å…¶ä»–å¤´éƒ¨å­—æ®µ
            (25, 31, 'ADDRESS', 'ADDRESSING'),  # æ ‘IDç­‰
        ]

        for start, end, sem_type, sem_func in field_mapping:
            for pos in range(start, min(end + 1, len(raw_bytes))):
                annotations['semantic_types'][str(pos)] = sem_type
                annotations['semantic_functions'][str(pos)] = sem_func

        # æ•°æ®éƒ¨åˆ†
        for i in range(32, len(raw_bytes)):
            annotations['semantic_types'][str(i)] = 'DATA'
            annotations['semantic_functions'][str(i)] = 'APPLICATION_DATA'

        boundaries = [b for b in boundaries if b < len(raw_bytes)]
        annotations['syntax_boundaries'] = sorted(boundaries)
        return annotations

    def _create_smb2_ground_truth(self, raw_bytes: bytes, label) -> Dict:
        """åˆ›å»ºSMB2 ground truth"""
        annotations = {'syntax_boundaries': [], 'semantic_types': {}, 'semantic_functions': {}}

        if len(raw_bytes) < 64:
            return annotations

        # SMB2å¤´éƒ¨ç»“æ„è¾¹ç•Œ
        boundaries = [0, 4, 6, 8, 12, 16, 24, 64]

        field_mapping = [
            (0, 3, 'HEADER', 'IDENTIFIER'),  # åè®®æ ‡è¯†
            (4, 5, 'LENGTH', 'DATA_LENGTH'),  # ç»“æ„å¤§å°
            (6, 7, 'COMMAND', 'CONTROL_CMD'),  # å‘½ä»¤
            (8, 11, 'FLAGS', 'CONTROL_CMD'),  # æ ‡å¿—
            (12, 15, 'HEADER', 'SESSION_MGMT'),  # çŠ¶æ€
            (16, 23, 'ADDRESS', 'ADDRESSING'),  # ä¼šè¯ID
            (24, 63, 'HEADER', 'SESSION_MGMT'),  # å…¶ä»–å¤´éƒ¨
        ]

        for start, end, sem_type, sem_func in field_mapping:
            for pos in range(start, min(end + 1, len(raw_bytes))):
                annotations['semantic_types'][str(pos)] = sem_type
                annotations['semantic_functions'][str(pos)] = sem_func

        # æ•°æ®éƒ¨åˆ†
        for i in range(64, len(raw_bytes)):
            annotations['semantic_types'][str(i)] = 'DATA'
            annotations['semantic_functions'][str(i)] = 'APPLICATION_DATA'

        boundaries = [b for b in boundaries if b < len(raw_bytes)]
        annotations['syntax_boundaries'] = sorted(boundaries)
        return annotations

    def _create_dns_ground_truth(self, raw_bytes: bytes, label) -> Dict:
        """åˆ›å»ºDNS ground truth"""
        annotations = {'syntax_boundaries': [], 'semantic_types': {}, 'semantic_functions': {}}

        if len(raw_bytes) < 12:
            return annotations

        # DNSå¤´éƒ¨ç»“æ„è¾¹ç•Œ
        boundaries = [0, 2, 4, 6, 8, 10, 12]

        field_mapping = [
            (0, 1, 'HEADER', 'IDENTIFIER'),  # äº‹åŠ¡ID
            (2, 3, 'FLAGS', 'CONTROL_CMD'),  # æ ‡å¿—
            (4, 5, 'LENGTH', 'DATA_LENGTH'),  # é—®é¢˜è®¡æ•°
            (6, 7, 'LENGTH', 'DATA_LENGTH'),  # å›ç­”è®¡æ•°
            (8, 9, 'LENGTH', 'DATA_LENGTH'),  # æƒå¨è®¡æ•°
            (10, 11, 'LENGTH', 'DATA_LENGTH'),  # é™„åŠ è®¡æ•°
        ]

        for start, end, sem_type, sem_func in field_mapping:
            for pos in range(start, min(end + 1, len(raw_bytes))):
                annotations['semantic_types'][str(pos)] = sem_type
                annotations['semantic_functions'][str(pos)] = sem_func

        # æŸ¥è¯¢/å›ç­”éƒ¨åˆ†
        for i in range(12, len(raw_bytes)):
            annotations['semantic_types'][str(i)] = 'DATA'
            annotations['semantic_functions'][str(i)] = 'APPLICATION_DATA'

        boundaries = [b for b in boundaries if b < len(raw_bytes)]
        annotations['syntax_boundaries'] = sorted(boundaries)
        return annotations

    def _create_ftp_ground_truth(self, raw_bytes: bytes, label) -> Dict:
        """åˆ›å»ºFTP ground truth"""
        annotations = {'syntax_boundaries': [], 'semantic_types': {}, 'semantic_functions': {}}

        # FTPæ˜¯æ–‡æœ¬åè®®ï¼Œä½¿ç”¨ç®€å•è¾¹ç•Œ
        boundaries = [0]
        if len(raw_bytes) > 4:
            boundaries.append(4)

        for i in range(len(raw_bytes)):
            annotations['semantic_types'][str(i)] = 'DATA'
            annotations['semantic_functions'][str(i)] = 'APPLICATION_DATA'

        boundaries = [b for b in boundaries if b < len(raw_bytes)]
        annotations['syntax_boundaries'] = sorted(boundaries)
        return annotations

    def _create_tls_ground_truth(self, raw_bytes: bytes, label) -> Dict:
        """åˆ›å»ºTLS ground truth"""
        annotations = {'syntax_boundaries': [], 'semantic_types': {}, 'semantic_functions': {}}

        if len(raw_bytes) < 5:
            return annotations

        # TLSè®°å½•ç»“æ„è¾¹ç•Œ
        boundaries = [0, 1, 3, 5]

        field_mapping = [
            (0, 0, 'HEADER', 'IDENTIFIER'),  # å†…å®¹ç±»å‹
            (1, 2, 'VERSION', 'PROTOCOL_SPECIFIC'),  # ç‰ˆæœ¬
            (3, 4, 'LENGTH', 'DATA_LENGTH'),  # é•¿åº¦
        ]

        for start, end, sem_type, sem_func in field_mapping:
            for pos in range(start, min(end + 1, len(raw_bytes))):
                annotations['semantic_types'][str(pos)] = sem_type
                annotations['semantic_functions'][str(pos)] = sem_func

        # åŠ å¯†æ•°æ®
        for i in range(5, len(raw_bytes)):
            annotations['semantic_types'][str(i)] = 'DATA'
            annotations['semantic_functions'][str(i)] = 'SECURITY'

        boundaries = [b for b in boundaries if b < len(raw_bytes)]
        annotations['syntax_boundaries'] = sorted(boundaries)
        return annotations

    def _create_dhcp_ground_truth(self, raw_bytes: bytes, label) -> Dict:
        """åˆ›å»ºDHCP ground truth"""
        annotations = {'syntax_boundaries': [], 'semantic_types': {}, 'semantic_functions': {}}

        if len(raw_bytes) < 240:
            return annotations

        # DHCPå›ºå®šå¤´éƒ¨ç»“æ„è¾¹ç•Œ
        boundaries = [0, 1, 2, 3, 4, 8, 10, 12, 28, 44, 60, 76, 140, 236, 240]

        field_mapping = [
            (0, 0, 'HEADER', 'IDENTIFIER'),  # æ¶ˆæ¯ç±»å‹
            (1, 1, 'HEADER', 'PROTOCOL_SPECIFIC'),  # ç¡¬ä»¶ç±»å‹
            (2, 2, 'LENGTH', 'DATA_LENGTH'),  # ç¡¬ä»¶åœ°å€é•¿åº¦
            (3, 3, 'HEADER', 'CONTROL_CMD'),  # è·³æ•°
            (4, 7, 'HEADER', 'IDENTIFIER'),  # äº‹åŠ¡ID
            (8, 9, 'TIMESTAMP', 'SESSION_MGMT'),  # ç§’æ•°
            (10, 11, 'FLAGS', 'CONTROL_CMD'),  # æ ‡å¿—
            (12, 27, 'ADDRESS', 'ADDRESSING'),  # IPåœ°å€å­—æ®µ
            (28, 43, 'ADDRESS', 'ADDRESSING'),  # æœåŠ¡å™¨IP
            (44, 59, 'ADDRESS', 'ADDRESSING'),  # ç½‘å…³IP
            (60, 75, 'ADDRESS', 'ADDRESSING'),  # å®¢æˆ·ç«¯ç¡¬ä»¶åœ°å€
            (76, 139, 'DATA', 'CONFIGURATION'),  # æœåŠ¡å™¨å
            (140, 235, 'DATA', 'CONFIGURATION'),  # å¯åŠ¨æ–‡ä»¶å
            (236, 239, 'HEADER', 'IDENTIFIER'),  # Magic Cookie
        ]

        for start, end, sem_type, sem_func in field_mapping:
            for pos in range(start, min(end + 1, len(raw_bytes))):
                annotations['semantic_types'][str(pos)] = sem_type
                annotations['semantic_functions'][str(pos)] = sem_func

        # é€‰é¡¹å­—æ®µ
        for i in range(240, len(raw_bytes)):
            annotations['semantic_types'][str(i)] = 'OPTION'
            annotations['semantic_functions'][str(i)] = 'CONFIGURATION'

        boundaries = [b for b in boundaries if b < len(raw_bytes)]
        annotations['syntax_boundaries'] = sorted(boundaries)
        return annotations

    def _create_generic_ground_truth(self, raw_bytes: bytes, label) -> Dict:
        """åˆ›å»ºé€šç”¨åè®®çš„ground truth"""
        annotations = {'syntax_boundaries': [], 'semantic_types': {}, 'semantic_functions': {}}

        # ç®€å•çš„é€šç”¨ç»“æ„ï¼šå›ºå®šé—´éš”è¾¹ç•Œ
        header_size = max(4, len(raw_bytes) // 4)
        boundaries = [0, header_size]

        if len(raw_bytes) > header_size * 2:
            boundaries.append(header_size * 2)

        # å¤´éƒ¨
        for i in range(min(header_size, len(raw_bytes))):
            annotations['semantic_types'][str(i)] = 'HEADER'
            annotations['semantic_functions'][str(i)] = 'IDENTIFIER'

        # æ•°æ®
        for i in range(header_size, len(raw_bytes)):
            annotations['semantic_types'][str(i)] = 'DATA'
            annotations['semantic_functions'][str(i)] = 'PAYLOAD'

        boundaries = [b for b in boundaries if b < len(raw_bytes)]
        annotations['syntax_boundaries'] = sorted(boundaries)
        return annotations


# ===========================
# åè®®æ— å…³çš„ç¼–ç å™¨ - ä¿æŒä¸å˜
# ===========================

class ProtocolAgnosticEncoder(nn.Module):
    """åè®®æ— å…³çš„ç‰¹å¾ç¼–ç å™¨"""

    def __init__(self, d_model: int = 512, num_layers: int = 6, num_heads: int = 8):
        super().__init__()
        self.d_model = d_model

        # é€šç”¨ç‰¹å¾æå–å±‚
        self.byte_embedding = nn.Embedding(256, d_model)
        self.positional_encoding = self._create_positional_encoding(512, d_model)

        # Transformerç¼–ç å™¨
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=num_heads,
            dim_feedforward=d_model * 4,
            dropout=0.1,
            activation='gelu',
            batch_first=True
        )
        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers)

        # åè®®æ— å…³çš„ç‰¹å¾æŠ•å½±å±‚
        self.feature_projector = nn.Sequential(
            nn.Linear(d_model, d_model),
            nn.LayerNorm(d_model),
            nn.GELU(),
            nn.Dropout(0.1),
            nn.Linear(d_model, d_model)
        )

    def _create_positional_encoding(self, max_len: int, d_model: int):
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() *
                             (-np.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        return nn.Parameter(pe.unsqueeze(0), requires_grad=False)

    def forward(self, x: torch.Tensor, attention_mask: Optional[torch.Tensor] = None):
        batch_size, seq_len = x.size()

        # ç¡®ä¿è¾“å…¥æ˜¯æ­£ç¡®çš„ç±»å‹
        x = x.long()  # ç¡®ä¿æ˜¯é•¿æ•´å‹

        # Byte embedding
        embedded = self.byte_embedding(x)

        # æ·»åŠ ä½ç½®ç¼–ç 
        if seq_len <= self.positional_encoding.size(1):
            embedded += self.positional_encoding[:, :seq_len, :]

        # Transformerç¼–ç 
        encoded = self.transformer_encoder(embedded, src_key_padding_mask=attention_mask)

        # åè®®æ— å…³ç‰¹å¾æŠ•å½±
        protocol_agnostic_features = self.feature_projector(encoded)

        return {
            'protocol_agnostic_features': protocol_agnostic_features,
            'raw_features': encoded,
            'embeddings': embedded
        }


# ===========================
# åè®®ç‰¹å®šä»»åŠ¡å¤´ - ä¿æŒä¸å˜
# ===========================

class ProtocolSpecificHead(nn.Module):
    """åè®®ç‰¹å®šçš„ä»»åŠ¡å¤´"""

    def __init__(self, d_model: int, num_semantic_types: int, num_semantic_functions: int,
                 protocol_name: str):
        super().__init__()
        self.protocol_name = protocol_name
        self.d_model = d_model

        # åè®®ç‰¹å®šçš„é€‚é…å±‚
        self.protocol_adapter = nn.Sequential(
            nn.Linear(d_model, d_model),
            nn.LayerNorm(d_model),
            nn.GELU(),
            nn.Dropout(0.1)
        )

        # è¾¹ç•Œæ£€æµ‹å¤´
        self.boundary_head = nn.Sequential(
            nn.Linear(d_model, d_model // 2),
            nn.GELU(),
            nn.Dropout(0.1),
            nn.Linear(d_model // 2, 2)
        )

        # è¯­ä¹‰ç±»å‹åˆ†ç±»å¤´
        self.semantic_type_head = nn.Sequential(
            nn.Linear(d_model, d_model // 2),
            nn.GELU(),
            nn.Dropout(0.1),
            nn.Linear(d_model // 2, num_semantic_types)
        )

        # è¯­ä¹‰åŠŸèƒ½åˆ†ç±»å¤´
        self.semantic_function_head = nn.Sequential(
            nn.Linear(d_model, d_model // 2),
            nn.GELU(),
            nn.Dropout(0.1),
            nn.Linear(d_model // 2, num_semantic_functions)
        )

    def forward(self, protocol_agnostic_features: torch.Tensor):
        # åè®®ç‰¹å®šé€‚é…
        adapted_features = self.protocol_adapter(protocol_agnostic_features)

        # å„ä»»åŠ¡é¢„æµ‹
        boundary_logits = self.boundary_head(adapted_features)
        semantic_type_logits = self.semantic_type_head(adapted_features)
        semantic_function_logits = self.semantic_function_head(adapted_features)

        return {
            'boundary_logits': boundary_logits,
            'semantic_type_logits': semantic_type_logits,
            'semantic_function_logits': semantic_function_logits,
            'adapted_features': adapted_features
        }


# ===========================
# è·¨åè®®è¿ç§»æ¨¡å‹ - ä¿æŒä¸å˜
# ===========================

class GenericCrossProtocolTransferModel(nn.Module):
    """é€šç”¨è·¨åè®®è¿ç§»å­¦ä¹ æ¨¡å‹ï¼Œæ”¯æŒåŠ¨æ€åè®®"""

    def __init__(self, protocol_names: List[str], d_model: int = 512, encoder_layers: int = 6,
                 num_semantic_types: int = 14, num_semantic_functions: int = 14):
        super().__init__()

        self.protocol_names = protocol_names

        # åè®®æ— å…³çš„ç¼–ç å™¨ï¼ˆå…±äº«å±‚ï¼‰
        self.protocol_agnostic_encoder = ProtocolAgnosticEncoder(
            d_model=d_model,
            num_layers=encoder_layers
        )

        # ä¸ºæ¯ä¸ªåè®®åˆ›å»ºç‰¹å®šçš„ä»»åŠ¡å¤´
        self.protocol_heads = nn.ModuleDict()
        for protocol_name in protocol_names:
            self.protocol_heads[protocol_name] = ProtocolSpecificHead(
                d_model, num_semantic_types, num_semantic_functions, protocol_name
            )

        # é€šç”¨ä»»åŠ¡å¤´ï¼ˆç”¨äºæœªçŸ¥åè®®ï¼‰
        self.protocol_heads['general'] = ProtocolSpecificHead(
            d_model, num_semantic_types, num_semantic_functions, 'general'
        )

        # åè®®åˆ†ç±»å™¨
        self.protocol_classifier = nn.Sequential(
            nn.Linear(d_model, d_model // 2),
            nn.GELU(),
            nn.Dropout(0.1),
            nn.Linear(d_model // 2, len(protocol_names) + 1)  # +1 for unknown
        )

    def add_protocol(self, protocol_name: str, num_semantic_types: int = 14,
                     num_semantic_functions: int = 14):
        """åŠ¨æ€æ·»åŠ æ–°åè®®æ”¯æŒ"""
        if protocol_name not in self.protocol_heads:
            self.protocol_heads[protocol_name] = ProtocolSpecificHead(
                self.protocol_agnostic_encoder.d_model,
                num_semantic_types,
                num_semantic_functions,
                protocol_name
            )
            self.protocol_names.append(protocol_name)
            print(f"æ¨¡å‹å·²æ·»åŠ å¯¹åè®® '{protocol_name}' çš„æ”¯æŒ")
        else:
            print(f"åè®® '{protocol_name}' å·²å­˜åœ¨")

    def forward(self, x: torch.Tensor, protocol: Optional[str] = None,
                attention_mask: Optional[torch.Tensor] = None,
                apply_boundary_postprocess: bool = False):
        """æ¨¡å‹å‰å‘ä¼ æ’­"""

        # æå–åè®®æ— å…³ç‰¹å¾
        encoder_outputs = self.protocol_agnostic_encoder(x, attention_mask)
        protocol_agnostic_features = encoder_outputs['protocol_agnostic_features']

        # åè®®åˆ†ç±»
        protocol_probs = self.protocol_classifier(
            protocol_agnostic_features.mean(dim=1)
        )

        # é€‰æ‹©å¯¹åº”çš„åè®®å¤´
        if protocol and protocol in self.protocol_heads:
            head_outputs = self.protocol_heads[protocol](protocol_agnostic_features)
        else:
            # ä½¿ç”¨é€šç”¨å¤´
            head_outputs = self.protocol_heads['general'](protocol_agnostic_features)

        return {
            **head_outputs,
            'protocol_probs': protocol_probs,
            'protocol_agnostic_features': protocol_agnostic_features
        }

    def freeze_encoder(self):
        """å†»ç»“ç¼–ç å™¨å‚æ•°"""
        for param in self.protocol_agnostic_encoder.parameters():
            param.requires_grad = False

    def unfreeze_encoder(self):
        """è§£å†»ç¼–ç å™¨å‚æ•°"""
        for param in self.protocol_agnostic_encoder.parameters():
            param.requires_grad = True


# ===========================
# æ•°æ®é›†ç±» - ä¿®å¤è¾¹ç•Œæ ‡ç­¾ç”Ÿæˆ
# ===========================

class GenericTransferLearningDataset(Dataset):
    """æ”¯æŒå¤šåè®®çš„é€šç”¨æ•°æ®é›† - ä¿®å¤è¾¹ç•Œå¤„ç†"""

    def __init__(self, data: List[Dict], max_length: int = 256,
                 protocol_filter: Optional[str] = None, augment: bool = False,
                 unified_semantic_types: List[str] = None,
                 unified_semantic_functions: List[str] = None):

        self.max_length = max_length
        self.augment = augment

        # å¦‚æœæŒ‡å®šåè®®è¿‡æ»¤å™¨ï¼Œåªä¿ç•™è¯¥åè®®çš„æ•°æ®
        if protocol_filter:
            self.data = [sample for sample in data if sample['protocol'] == protocol_filter]
            print(f"è¿‡æ»¤åˆ°{protocol_filter}åè®®æ•°æ®: {len(self.data)}æ¡")
        else:
            self.data = data

        # ä½¿ç”¨æä¾›çš„ç»Ÿä¸€è¯­ä¹‰æ ‡ç­¾æˆ–é»˜è®¤æ ‡ç­¾
        self.unified_semantic_types = unified_semantic_types or [
            'PADDING', 'HEADER', 'ADDRESS', 'COMMAND', 'LENGTH',
            'DATA', 'CHECKSUM', 'CONTROL', 'FUNCTION', 'OPTION',
            'TIMESTAMP', 'VERSION', 'FLAGS', 'PAYLOAD'
        ]

        self.unified_semantic_functions = unified_semantic_functions or [
            'UNKNOWN', 'IDENTIFIER', 'ADDRESSING', 'CONTROL_CMD',
            'DATA_LENGTH', 'PAYLOAD', 'VALIDATION', 'RESERVED',
            'PROTOCOL_SPECIFIC', 'CONFIGURATION', 'SESSION_MGMT',
            'SECURITY', 'ROUTING', 'APPLICATION_DATA'
        ]

        # åˆ›å»ºç¼–ç å™¨
        self.type_encoder = LabelEncoder()
        self.function_encoder = LabelEncoder()

        self.type_encoder.fit(self.unified_semantic_types)
        self.function_encoder.fit(self.unified_semantic_functions)

        # ç»Ÿè®¡åè®®åˆ†å¸ƒ
        protocol_dist = Counter([sample['protocol'] for sample in self.data])
        print(f"æ•°æ®é›†åè®®åˆ†å¸ƒ: {dict(protocol_dist)}")

        # ã€æ–°å¢ã€‘è¾¹ç•Œè´¨é‡ç»Ÿè®¡
        self._analyze_boundary_quality()

    def _analyze_boundary_quality(self):
        """åˆ†æè¾¹ç•Œæ•°æ®è´¨é‡"""
        boundary_stats = {
            'valid_boundaries': 0,
            'empty_boundaries': 0,
            'avg_boundaries_per_sample': 0,
            'total_samples': len(self.data)
        }

        total_boundaries = 0
        for sample in self.data:
            boundaries = sample['ground_truth']['syntax_boundaries']
            if len(boundaries) > 0:
                boundary_stats['valid_boundaries'] += 1
                total_boundaries += len(boundaries)
            else:
                boundary_stats['empty_boundaries'] += 1

        if boundary_stats['valid_boundaries'] > 0:
            boundary_stats['avg_boundaries_per_sample'] = total_boundaries / boundary_stats['valid_boundaries']

        print(f"è¾¹ç•Œè´¨é‡ç»Ÿè®¡: {boundary_stats}")

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        sample = self.data[idx]

        try:
            raw_bytes = bytes.fromhex(sample['raw_data'])
        except:
            raw_bytes = b'\x00'

        # ç¡®ä¿æ•°æ®ç±»å‹ä¸ºfloat32
        byte_sequence = np.array(list(raw_bytes), dtype=np.float32)

        # æ•°æ®å¢å¼º
        if self.augment and random.random() < 0.1:
            noise = np.random.normal(0, 0.5, byte_sequence.shape).astype(np.float32)
            byte_sequence = np.clip(byte_sequence + noise, 0, 255)

        # è®°å½•åŸå§‹é•¿åº¦ç”¨äºperfectionè®¡ç®—
        original_length = len(byte_sequence)

        # å¡«å……æˆ–æˆªæ–­
        if len(byte_sequence) > self.max_length:
            byte_sequence = byte_sequence[:self.max_length]
            actual_length = self.max_length
        else:
            actual_length = len(byte_sequence)
            byte_sequence = np.pad(byte_sequence,
                                   (0, self.max_length - len(byte_sequence)), 'constant')

        # åˆ›å»ºæ ‡ç­¾
        ground_truth = sample['ground_truth']

        # ã€ä¿®å¤ã€‘è¾¹ç•Œæ ‡ç­¾ç”Ÿæˆ - é‡è¦ä¿®å¤
        boundary_labels = np.zeros(self.max_length, dtype=np.int64)
        valid_boundaries = []

        for boundary in ground_truth['syntax_boundaries']:
            if 0 <= boundary < actual_length:  # åªè€ƒè™‘å®é™…é•¿åº¦å†…çš„è¾¹ç•Œ
                boundary_labels[boundary] = 1
                valid_boundaries.append(boundary)

        # ã€æ–°å¢ã€‘ç¡®ä¿è‡³å°‘æœ‰ä¸€äº›è¾¹ç•Œæ ‡è®°
        if len(valid_boundaries) == 0 and actual_length > 0:
            # å¦‚æœæ²¡æœ‰æœ‰æ•ˆè¾¹ç•Œï¼Œåœ¨åºåˆ—å¼€å§‹å¤„æ ‡è®°ä¸€ä¸ªè¾¹ç•Œ
            boundary_labels[0] = 1
            valid_boundaries.append(0)

        # è¯­ä¹‰ç±»å‹æ ‡ç­¾ - ç¡®ä¿æ•°æ®ç±»å‹ä¸ºint64
        type_labels = np.zeros(self.max_length, dtype=np.int64)
        for pos_str, type_name in ground_truth['semantic_types'].items():
            try:
                pos = int(pos_str)
                if 0 <= pos < actual_length and type_name in self.unified_semantic_types:
                    type_idx = self.type_encoder.transform([type_name])[0]
                    type_labels[pos] = type_idx
            except:
                continue

        # è¯­ä¹‰åŠŸèƒ½æ ‡ç­¾ - ç¡®ä¿æ•°æ®ç±»å‹ä¸ºint64
        function_labels = np.zeros(self.max_length, dtype=np.int64)
        for pos_str, func_name in ground_truth['semantic_functions'].items():
            try:
                pos = int(pos_str)
                if 0 <= pos < actual_length and func_name in self.unified_semantic_functions:
                    func_idx = self.function_encoder.transform([func_name])[0]
                    function_labels[pos] = func_idx
            except:
                continue

        return {
            'sequence': torch.tensor(byte_sequence, dtype=torch.float32),
            'boundary_labels': torch.tensor(boundary_labels, dtype=torch.long),
            'semantic_type_labels': torch.tensor(type_labels, dtype=torch.long),
            'semantic_function_labels': torch.tensor(function_labels, dtype=torch.long),
            'actual_length': actual_length,  # ã€ä¿®å¤ã€‘ç¡®ä¿æä¾›æ­£ç¡®çš„å®é™…é•¿åº¦
            'protocol': sample['protocol'],
            'valid_boundaries_count': len(valid_boundaries),  # è°ƒè¯•ä¿¡æ¯
            'original_length': original_length  # ã€æ–°å¢ã€‘åŸå§‹é•¿åº¦ä¿¡æ¯
        }


# ===========================
# è®­ç»ƒå™¨ - ä¿®å¤perfectionè®¡ç®—
# ===========================

class GenericTransferLearningTrainer:
    """é€šç”¨è·¨åè®®è¿ç§»å­¦ä¹ è®­ç»ƒå™¨ - ä¿®å¤perfectionè®¡ç®—"""

    def __init__(self, model: GenericCrossProtocolTransferModel, device: str = 'cpu',
                 protocol_names: List[str] = None):
        self.model = model.to(device)
        self.device = device
        self.protocol_names = protocol_names or []
        self.training_history = defaultdict(list)

        # è¯­ä¹‰åˆ†ç±»æŸå¤±
        self.semantic_criterion = nn.CrossEntropyLoss(ignore_index=0)
        self.protocol_criterion = nn.CrossEntropyLoss()

        # è¾¹ç•Œæ£€æµ‹æŸå¤± - å°†åœ¨è®­ç»ƒæ—¶åŠ¨æ€è®¡ç®—æƒé‡
        self.boundary_criterion = None

        # å½“å‰é˜¶æ®µ
        self.current_stage = 'initialization'

    def _boundaries_to_fields(self, boundaries: List[int], sequence_length: int) -> List[Tuple[int, int]]:
        """å°†è¾¹ç•Œä½ç½®è½¬æ¢ä¸ºå­—æ®µèŒƒå›´åˆ—è¡¨

        Args:
            boundaries: è¾¹ç•Œä½ç½®åˆ—è¡¨ï¼Œå¦‚ [0, 2, 4, 6, 7, 8]
            sequence_length: åºåˆ—æ€»é•¿åº¦

        Returns:
            å­—æ®µèŒƒå›´åˆ—è¡¨ï¼Œå¦‚ [(0, 1), (2, 3), (4, 5), (6, 6), (7, 7), (8, sequence_length-1)]
        """
        if not boundaries:
            return [(0, sequence_length - 1)] if sequence_length > 0 else []

        fields = []
        boundaries = sorted(set(boundaries))  # å»é‡å¹¶æ’åº

        for i in range(len(boundaries)):
            start = boundaries[i]

            if i < len(boundaries) - 1:
                # ä¸æ˜¯æœ€åä¸€ä¸ªè¾¹ç•Œ
                end = boundaries[i + 1] - 1
            else:
                # æœ€åä¸€ä¸ªè¾¹ç•Œï¼Œå­—æ®µå»¶ç»­åˆ°åºåˆ—æœ«å°¾
                end = sequence_length - 1

            if start <= end:  # ç¡®ä¿å­—æ®µæœ‰æ•ˆ
                fields.append((start, end))

        return fields

    def _calculate_boundary_weights(self, data_loader: DataLoader) -> torch.Tensor:
        """åŠ¨æ€è®¡ç®—è¾¹ç•Œæ£€æµ‹çš„ç±»åˆ«æƒé‡ - æ”¹è¿›ç‰ˆæœ¬"""
        print("ğŸ”„ è®¡ç®—è¾¹ç•Œæ£€æµ‹æƒé‡...")

        boundary_counts = [0, 0]  # [éè¾¹ç•Œ, è¾¹ç•Œ]
        total_sequences = 0
        avg_boundaries_per_seq = 0

        for batch in data_loader:
            boundary_labels = batch['boundary_labels'].numpy()
            total_sequences += boundary_labels.shape[0]

            for seq in boundary_labels:
                unique, counts = np.unique(seq, return_counts=True)
                for val, count in zip(unique, counts):
                    if val in [0, 1]:
                        boundary_counts[val] += count

                # ç»Ÿè®¡æ¯ä¸ªåºåˆ—çš„è¾¹ç•Œæ•°
                avg_boundaries_per_seq += np.sum(seq == 1)

        avg_boundaries_per_seq /= total_sequences if total_sequences > 0 else 1

        # ä½¿ç”¨æ¸©å’Œçš„æƒé‡ç­–ç•¥
        if boundary_counts[1] > 0:
            raw_ratio = boundary_counts[0] / boundary_counts[1]
            # é™åˆ¶æœ€å¤§æƒé‡ï¼Œé˜²æ­¢è¿‡åº¦é¢„æµ‹
            max_weight = 15.0 if avg_boundaries_per_seq < 8 else 10.0
            weight_ratio = min(max_weight, raw_ratio)
            weights = torch.tensor([1.0, weight_ratio], dtype=torch.float32, device=self.device)
        else:
            weights = torch.tensor([1.0, 10.0], dtype=torch.float32, device=self.device)

        print(f"  è¾¹ç•Œç»Ÿè®¡: éè¾¹ç•Œ={boundary_counts[0]}, è¾¹ç•Œ={boundary_counts[1]}")
        print(f"  å¹³å‡æ¯åºåˆ—è¾¹ç•Œæ•°: {avg_boundaries_per_seq:.1f}")
        print(f"  è¾¹ç•Œæ¯”ä¾‹: {boundary_counts[1] / sum(boundary_counts) * 100:.2f}%")
        print(f"  è®¡ç®—æƒé‡: {weights.tolist()}")

        return weights

    def _compute_loss(self, outputs, batch, protocol_name, data_loader=None):
        """è®¡ç®—æŸå¤± - æ”¹è¿›ç‰ˆæœ¬"""

        # å¦‚æœè¾¹ç•ŒæŸå¤±å‡½æ•°æœªåˆå§‹åŒ–ï¼Œåˆ™åŠ¨æ€è®¡ç®—æƒé‡
        if self.boundary_criterion is None and data_loader is not None:
            weights = self._calculate_boundary_weights(data_loader)
            self.boundary_criterion = nn.CrossEntropyLoss(weight=weights)
        elif self.boundary_criterion is None:
            # é»˜è®¤æ›´å¼ºçš„æƒé‡
            weights = torch.tensor([1.0, 50.0], dtype=torch.float32, device=self.device)
            self.boundary_criterion = nn.CrossEntropyLoss(weight=weights)

        # ã€æ–°å¢ã€‘è¾¹ç•Œå¯†åº¦æ­£åˆ™åŒ–æŸå¤±
        def boundary_density_loss(boundary_logits, boundary_labels, actual_lengths):
            """è¾¹ç•Œå¯†åº¦æ­£åˆ™åŒ–ï¼šæƒ©ç½šè¿‡åº¦é¢„æµ‹è¾¹ç•Œ"""
            boundary_probs = torch.softmax(boundary_logits, dim=-1)[:, :, 1]  # è¾¹ç•Œæ¦‚ç‡

            total_penalty = 0.0
            batch_size = boundary_probs.size(0)

            for i in range(batch_size):
                actual_len = actual_lengths[i] if hasattr(actual_lengths, '__len__') else actual_lengths
                actual_len = min(int(actual_len), boundary_probs.size(1))

                # è®¡ç®—é¢„æµ‹è¾¹ç•Œå¯†åº¦
                pred_boundary_density = torch.mean(boundary_probs[i, :actual_len])

                # è®¡ç®—çœŸå®è¾¹ç•Œå¯†åº¦
                true_boundary_density = torch.mean(boundary_labels[i, :actual_len].float())

                # æƒ©ç½šè¿‡åº¦é¢„æµ‹ï¼ˆé¢„æµ‹å¯†åº¦è¿œé«˜äºçœŸå®å¯†åº¦ï¼‰
                if pred_boundary_density > true_boundary_density * 2:  # å¦‚æœé¢„æµ‹å¯†åº¦æ˜¯çœŸå®çš„2å€ä»¥ä¸Š
                    penalty = (pred_boundary_density - true_boundary_density) ** 2
                    total_penalty += penalty

            return total_penalty / batch_size

        # æ ‡å‡†è¾¹ç•Œæ£€æµ‹æŸå¤±
        boundary_loss = self.boundary_criterion(
            outputs['boundary_logits'].view(-1, 2),
            batch['boundary_labels'].view(-1)
        )

        # ã€æ–°å¢ã€‘è¾¹ç•Œå¯†åº¦æ­£åˆ™åŒ–
        density_penalty = boundary_density_loss(
            outputs['boundary_logits'],
            batch['boundary_labels'],
            batch.get('actual_length', batch['boundary_labels'].size(1))
        )

        # è¯­ä¹‰ç±»å‹åˆ†ç±»æŸå¤±
        type_loss = self.semantic_criterion(
            outputs['semantic_type_logits'].view(-1, outputs['semantic_type_logits'].size(-1)),
            batch['semantic_type_labels'].view(-1)
        )

        # è¯­ä¹‰åŠŸèƒ½åˆ†ç±»æŸå¤±
        func_loss = self.semantic_criterion(
            outputs['semantic_function_logits'].view(-1, outputs['semantic_function_logits'].size(-1)),
            batch['semantic_function_labels'].view(-1)
        )

        # åè®®åˆ†ç±»æŸå¤±
        protocol_ids = self._get_protocol_ids(batch['protocol'])
        protocol_loss = self.protocol_criterion(outputs['protocol_probs'], protocol_ids)

        # ã€ä¿®æ”¹ã€‘ç»„åˆæŸå¤± - å¢åŠ å¯†åº¦æ­£åˆ™åŒ–
        total_loss = (
                5.0 * boundary_loss +  # å¢åŠ è¾¹ç•Œæ£€æµ‹æƒé‡
                2.0 * density_penalty +  # æ–°å¢ï¼šè¾¹ç•Œå¯†åº¦æ­£åˆ™åŒ–
                2.0 * type_loss +  # è¯­ä¹‰ç±»å‹åˆ†ç±»æƒé‡
                2.0 * func_loss +  # è¯­ä¹‰åŠŸèƒ½åˆ†ç±»æƒé‡
                0.3 * protocol_loss  # åè®®åˆ†ç±»æƒé‡è¾ƒä½
        )

        return total_loss

    def _evaluate_on_protocol(self, data_loader: DataLoader, protocol: str) -> Dict:
        """åœ¨ç‰¹å®šåè®®ä¸Šè¯„ä¼°æ¨¡å‹ - ä¿®æ­£ç‰ˆæœ¬ï¼Œå®ç°æ­£ç¡®çš„è¯„ä¼°æŒ‡æ ‡"""
        self.model.eval()

        all_type_preds = []
        all_type_labels = []
        all_func_preds = []
        all_func_labels = []

        # å­—æ®µçº§åˆ«çš„ç»Ÿè®¡
        total_true_fields = 0
        total_predicted_boundaries = 0
        total_true_positive_boundaries = 0
        perfectly_inferred_fields = 0

        # ä½ç½®çº§åˆ«çš„ç»Ÿè®¡
        all_boundary_preds = []
        all_boundary_labels = []

        # åºåˆ—çº§åˆ«çš„ç»Ÿè®¡ï¼ˆä¿ç•™åŸæœ‰çš„åºåˆ—å®Œç¾åŒ¹é…ï¼‰
        total_sequences = 0
        perfect_sequences = 0
        debug_info = []

        with torch.no_grad():
            for batch_idx, batch in enumerate(data_loader):
                batch = {k: v.to(self.device) if torch.is_tensor(v) else v
                         for k, v in batch.items()}

                outputs = self.model(batch['sequence'], protocol=protocol)

                # è·å–å®é™…é•¿åº¦
                actual_lengths = batch.get('actual_length', torch.sum(batch['sequence'] != 0, dim=1))

                # è¾¹ç•Œé¢„æµ‹ï¼ˆåŸå§‹é¢„æµ‹ï¼Œä¸ä½¿ç”¨åå¤„ç†ï¼‰
                boundary_pred = torch.argmax(outputs['boundary_logits'], dim=-1)

                # è¯­ä¹‰é¢„æµ‹ç»“æœ
                type_pred = torch.argmax(outputs['semantic_type_logits'], dim=-1)
                func_pred = torch.argmax(outputs['semantic_function_logits'], dim=-1)

                # æ”¶é›†è¯­ä¹‰é¢„æµ‹ç»“æœ
                all_type_preds.extend(type_pred.cpu().numpy().flatten())
                all_type_labels.extend(batch['semantic_type_labels'].cpu().numpy().flatten())
                all_func_preds.extend(func_pred.cpu().numpy().flatten())
                all_func_labels.extend(batch['semantic_function_labels'].cpu().numpy().flatten())

                # å¤„ç†è¾¹ç•Œæ£€æµ‹ç»“æœ
                batch_size = boundary_pred.size(0)
                seq_length = boundary_pred.size(1)

                boundary_pred_np = boundary_pred.cpu().numpy().astype(np.int64)
                boundary_labels_np = batch['boundary_labels'].cpu().numpy().astype(np.int64)

                # è·å–å®é™…åºåˆ—é•¿åº¦
                if 'actual_length' in batch:
                    actual_lengths_np = batch['actual_length']
                    if torch.is_tensor(actual_lengths_np):
                        actual_lengths_np = actual_lengths_np.cpu().numpy()
                else:
                    actual_lengths_np = []
                    for i in range(batch_size):
                        seq_data = batch['sequence'][i].cpu().numpy()
                        non_zero_indices = np.nonzero(seq_data)[0]
                        if len(non_zero_indices) > 0:
                            actual_lengths_np.append(non_zero_indices[-1] + 1)
                        else:
                            actual_lengths_np.append(seq_length)
                    actual_lengths_np = np.array(actual_lengths_np)

                for i in range(batch_size):
                    total_sequences += 1

                    # è·å–å®é™…åºåˆ—é•¿åº¦
                    if isinstance(actual_lengths_np, (list, np.ndarray)) and len(actual_lengths_np) > i:
                        actual_len = int(actual_lengths_np[i])
                    else:
                        actual_len = seq_length

                    actual_len = min(actual_len, seq_length)
                    actual_len = max(1, actual_len)

                    # è·å–å®é™…é•¿åº¦å†…çš„é¢„æµ‹å’Œæ ‡ç­¾
                    seq_pred = boundary_pred_np[i][:actual_len].astype(np.int64)
                    seq_label = boundary_labels_np[i][:actual_len].astype(np.int64)

                    # ä½ç½®çº§åˆ«ç»Ÿè®¡
                    all_boundary_preds.extend(seq_pred)
                    all_boundary_labels.extend(seq_label)

                    # åºåˆ—çº§åˆ«å®Œç¾åŒ¹é…ï¼ˆä¿ç•™åŸæœ‰å®šä¹‰ï¼‰
                    is_sequence_perfect = np.array_equal(seq_pred, seq_label)
                    if is_sequence_perfect:
                        perfect_sequences += 1

                    # ==== å­—æ®µçº§åˆ«çš„perfectionè®¡ç®— ====

                    # 1. ä»è¾¹ç•Œä½ç½®è½¬æ¢ä¸ºå­—æ®µ
                    true_boundaries = np.where(seq_label == 1)[0].tolist()
                    pred_boundaries = np.where(seq_pred == 1)[0].tolist()

                    # 2. ç»Ÿè®¡é¢„æµ‹çš„è¾¹ç•Œæ•°é‡
                    total_predicted_boundaries += len(pred_boundaries)

                    # 3. ç»Ÿè®¡çœŸæ­£é¢„æµ‹æ­£ç¡®çš„è¾¹ç•Œæ•°é‡ï¼ˆprecisionè®¡ç®—ç”¨ï¼‰
                    for pred_boundary in pred_boundaries:
                        if pred_boundary in true_boundaries:
                            total_true_positive_boundaries += 1

                    # 4. å°†è¾¹ç•Œè½¬æ¢ä¸ºå­—æ®µèŒƒå›´
                    true_fields = self._boundaries_to_fields(true_boundaries, actual_len)
                    pred_fields = self._boundaries_to_fields(pred_boundaries, actual_len)

                    # 5. ç»Ÿè®¡æ€»çš„çœŸå®å­—æ®µæ•°
                    total_true_fields += len(true_fields)

                    # 6. æ£€æŸ¥æ¯ä¸ªçœŸå®å­—æ®µæ˜¯å¦è¢«å®Œç¾æ¨æ–­
                    for true_field in true_fields:
                        if true_field in pred_fields:
                            perfectly_inferred_fields += 1

                    # è°ƒè¯•ä¿¡æ¯æ”¶é›†
                    if len(debug_info) < 5:
                        debug_info.append({
                            'batch_idx': batch_idx,
                            'seq_idx': i,
                            'actual_len': actual_len,
                            'true_boundaries': true_boundaries,
                            'pred_boundaries': pred_boundaries,
                            'true_fields': true_fields,
                            'pred_fields': pred_fields,
                            'is_sequence_perfect': is_sequence_perfect,
                            'field_matches': sum(1 for tf in true_fields if tf in pred_fields)
                        })

        # è®¡ç®—è¯­ä¹‰åˆ†ææŒ‡æ ‡
        type_mask = np.array(all_type_labels) != 0
        func_mask = np.array(all_func_labels) != 0

        type_f1 = f1_score(
            np.array(all_type_labels)[type_mask],
            np.array(all_type_preds)[type_mask],
            average='weighted', zero_division=0
        ) if type_mask.sum() > 0 else 0.0

        func_f1 = f1_score(
            np.array(all_func_labels)[func_mask],
            np.array(all_func_preds)[func_mask],
            average='weighted', zero_division=0
        ) if func_mask.sum() > 0 else 0.0

        # è®¡ç®—è¾¹ç•Œæ£€æµ‹æŒ‡æ ‡
        all_boundary_labels = np.array(all_boundary_labels, dtype=np.int64)
        all_boundary_preds = np.array(all_boundary_preds, dtype=np.int64)

        # 1. accuracy: æ­£ç¡®æ¨æ–­çš„ä½ç½®æ•° / æ‰€æœ‰åç§»ä½ç½®æ•°
        boundary_accuracy = accuracy_score(all_boundary_labels, all_boundary_preds)

        # 2. F1-score: è¾¹ç•Œæ£€æµ‹çš„äºŒåˆ†ç±»F1åˆ†æ•°
        boundary_f1 = f1_score(
            all_boundary_labels, all_boundary_preds,
            average='binary', pos_label=1, zero_division=0
        )

        # 3. perfection: å®Œç¾æ¨æ–­çš„å­—æ®µæ•° / æ‰€æœ‰çœŸå®å­—æ®µæ•°
        field_perfection = (perfectly_inferred_fields / total_true_fields
                            if total_true_fields > 0 else 0.0)

        # 4. Precision (ä¿ç•™ç”¨äºè°ƒè¯•)
        boundary_precision = (total_true_positive_boundaries / total_predicted_boundaries
                              if total_predicted_boundaries > 0 else 0.0)

        # 5. åºåˆ—çº§åˆ«çš„å®Œç¾åŒ¹é…ç‡ï¼ˆä¿ç•™åŸæœ‰å®šä¹‰ï¼‰
        sequence_perfection = perfect_sequences / total_sequences if total_sequences > 0 else 0.0

        # ç»Ÿè®¡ä¿¡æ¯
        total_boundaries_true = int(np.sum(all_boundary_labels))
        total_boundaries_pred = int(np.sum(all_boundary_preds))

        # è¯¦ç»†è°ƒè¯•ä¿¡æ¯è¾“å‡º
        if total_sequences > 0:
            print(f"    ä¿®æ­£è¯„ä¼°ç»Ÿè®¡:")
            print(f"      - æ€»åºåˆ—æ•°: {total_sequences}")
            print(f"      - æ€»çœŸå®å­—æ®µæ•°: {total_true_fields}")
            print(f"      - å®Œç¾æ¨æ–­å­—æ®µæ•°: {perfectly_inferred_fields}")
            print(f"      - æ€»é¢„æµ‹è¾¹ç•Œæ•°: {total_predicted_boundaries}")
            print(f"      - æ­£ç¡®é¢„æµ‹è¾¹ç•Œæ•°: {total_true_positive_boundaries}")
            print(f"      - åºåˆ—å®Œç¾åŒ¹é…æ•°: {perfect_sequences}")
            print(f"      - å­—æ®µå®Œç¾ç‡: {field_perfection:.4f}")
            print(f"      - è¾¹ç•ŒF1åˆ†æ•°: {boundary_f1:.4f}")

            # è¾“å‡ºå‰å‡ ä¸ªæ ·æœ¬çš„è¯¦ç»†ä¿¡æ¯
            if debug_info:
                print(f"      - æ ·æœ¬è¯¦ç»†ä¿¡æ¯:")
                for info in debug_info[:3]:
                    print(f"        æ ·æœ¬{info['seq_idx']}: é•¿åº¦={info['actual_len']}")
                    print(f"          çœŸå®è¾¹ç•Œ: {info['true_boundaries']} -> å­—æ®µ: {info['true_fields']}")
                    print(f"          é¢„æµ‹è¾¹ç•Œ: {info['pred_boundaries']} -> å­—æ®µ: {info['pred_fields']}")
                    print(f"          å­—æ®µåŒ¹é…: {info['field_matches']}/{len(info['true_fields'])}")

        return {
            'type_f1': type_f1,
            'func_f1': func_f1,
            # ä¿®æ­£çš„è¾¹ç•Œæ£€æµ‹æŒ‡æ ‡ - ç¬¦åˆè®ºæ–‡å®šä¹‰
            'boundary_acc': boundary_accuracy,  # accuracy (ä½ç½®çº§åˆ«å‡†ç¡®ç‡)
            'boundary_f1': boundary_f1,  # F1-score (è¾¹ç•Œæ£€æµ‹F1åˆ†æ•°)
            'boundary_perfection': field_perfection,  # perfection (å­—æ®µçº§åˆ«å®Œç¾ç‡)
            # ä¿ç•™ç”¨äºè°ƒè¯•
            'boundary_precision': boundary_precision,  # precision (è¾¹ç•Œé¢„æµ‹ç²¾ç¡®ç‡)
            'sequence_perfection': sequence_perfection,  # åºåˆ—çº§åˆ«å®Œç¾ç‡ï¼ˆä¿ç•™ï¼‰
            # å…¼å®¹æ€§ï¼šä½¿ç”¨æ–°çš„å­—æ®µå®Œç¾ç‡ä½œä¸ºä¸»è¦çš„perfectionæŒ‡æ ‡
            'field_perfection': field_perfection,  # å­—æ®µçº§åˆ«å®Œç¾ç‡
            # ç»Ÿè®¡ä¿¡æ¯
            'boundary_stats': {
                'total_boundaries_true': total_boundaries_true,
                'total_boundaries_pred': total_boundaries_pred,
                'total_sequences': total_sequences,
                'perfect_sequences': perfect_sequences,
                'total_true_fields': total_true_fields,
                'perfectly_inferred_fields': perfectly_inferred_fields,
                'total_predicted_boundaries': total_predicted_boundaries,
                'total_true_positive_boundaries': total_true_positive_boundaries,
                'debug_samples': debug_info[:5]
            }
        }

    def transfer_to_target(self, target_loader: DataLoader, val_loader: DataLoader,
                           target_protocol: str, epochs: int = 20, lr: float = 5e-5,
                           freeze_encoder: bool = True):
        """è¿ç§»åˆ°ç›®æ ‡åè®® - ä½¿ç”¨ä¿®æ­£çš„è¯„ä¼°æŒ‡æ ‡"""
        print(f"\né˜¶æ®µ2ï¼šè¿ç§»åˆ° {target_protocol.upper()} åè®®...")
        self.current_stage = 'target_transfer'

        # ç¡®ä¿æ¨¡å‹æ”¯æŒç›®æ ‡åè®®
        if target_protocol not in self.model.protocol_heads:
            self.model.add_protocol(target_protocol)

        # é‡ç½®è¾¹ç•ŒæŸå¤±å‡½æ•°ä»¥é‡æ–°è®¡ç®—æƒé‡
        self.boundary_criterion = None

        if freeze_encoder:
            print("å†»ç»“ç¼–ç å™¨ï¼Œä»…è®­ç»ƒåè®®ç‰¹å®šå¤´")
            self.model.freeze_encoder()
        else:
            print("ç«¯åˆ°ç«¯å¾®è°ƒ")
            self.model.unfreeze_encoder()

        # é…ç½®ä¼˜åŒ–å™¨
        if freeze_encoder:
            optimizer = optim.AdamW([
                {'params': self.model.protocol_heads[target_protocol].parameters(), 'lr': lr * 2},
                {'params': self.model.protocol_classifier.parameters(), 'lr': lr}
            ], weight_decay=0.01)
        else:
            optimizer = optim.AdamW([
                {'params': self.model.protocol_agnostic_encoder.parameters(), 'lr': lr * 0.1},
                {'params': self.model.protocol_heads[target_protocol].parameters(), 'lr': lr},
                {'params': self.model.protocol_classifier.parameters(), 'lr': lr}
            ], weight_decay=0.01)

        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)

        best_f1 = 0.0
        best_boundary_f1 = 0.0
        best_field_perfection = 0.0
        patience = 0
        max_patience = 8

        print(f"ä¿®æ­£çš„Format ExtractionæŒ‡æ ‡è¯´æ˜:")
        print(f"  - accuracy: ä½ç½®çº§åˆ«çš„è¾¹ç•Œæ£€æµ‹å‡†ç¡®ç‡ (æ­£ç¡®é¢„æµ‹ä½ç½®æ•°/æ€»ä½ç½®æ•°)")
        print(f"  - F1-score: è¾¹ç•Œæ£€æµ‹çš„äºŒåˆ†ç±»F1åˆ†æ•°")
        print(f"  - perfection: å­—æ®µçº§åˆ«å®Œç¾ç‡ (å®Œç¾æ¨æ–­å­—æ®µæ•°/æ€»çœŸå®å­—æ®µæ•°)")

        for epoch in range(epochs):
            self.model.train()
            total_loss = 0.0

            for batch in target_loader:
                batch = {k: v.to(self.device) if torch.is_tensor(v) else v
                         for k, v in batch.items()}

                optimizer.zero_grad()

                outputs = self.model(batch['sequence'], protocol=target_protocol)
                loss = self._compute_loss(outputs, batch, target_protocol, target_loader)

                loss.backward()
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
                optimizer.step()

                total_loss += loss.item()

            scheduler.step()

            # éªŒè¯ - ä½¿ç”¨ä¿®æ­£çš„è¯„ä¼°æ–¹æ³•
            val_metrics = self._evaluate_on_protocol(val_loader, target_protocol)
            avg_f1 = (val_metrics['type_f1'] + val_metrics['func_f1']) / 2
            boundary_f1 = val_metrics['boundary_f1']
            field_perfection = val_metrics['field_perfection']

            if avg_f1 > best_f1:
                best_f1 = avg_f1
                best_boundary_f1 = boundary_f1
                best_field_perfection = field_perfection
                patience = 0
                stage_name = 'frozen' if freeze_encoder else 'finetuned'
                self._save_checkpoint(f'{target_protocol}_{stage_name}.pth', epoch, best_f1)
                print(f'æ–°æœ€ä½³ç»“æœ - Overall F1: {best_f1:.4f}')
            else:
                patience += 1

            if epoch % 3 == 0:
                print(f'  Epoch {epoch}/{epochs}: Loss={total_loss / len(target_loader):.4f}')
                print(
                    f'    Overall F1={avg_f1:.4f}, Type F1={val_metrics["type_f1"]:.4f}, Func F1={val_metrics["func_f1"]:.4f}')
                print(
                    f'    Format Extraction - accuracy={val_metrics["boundary_acc"]:.4f}, F1-score={boundary_f1:.4f}, perfection={field_perfection:.4f}')
                print(
                    f'    (Precision={val_metrics["boundary_precision"]:.4f}, Seq.Perfect={val_metrics["sequence_perfection"]:.4f})')

            if patience >= max_patience:
                print(f"æ—©åœï¼š{max_patience}ä¸ªepochæ— æ”¹å–„")
                break

        print(f"ç›®æ ‡åè®®è¿ç§»å®Œæˆ:")
        print(f"  - æœ€ä½³Overall F1: {best_f1:.4f}")
        print(f"  - æœ€ä½³Format ExtractionæŒ‡æ ‡:")
        print(f"    * F1-score: {best_boundary_f1:.4f}")
        print(f"    * perfection: {best_field_perfection:.4f}")

        return best_f1

    def train_source_protocols(self, source_loaders: Dict[str, DataLoader],
                               epochs: int = 30, lr: float = 1e-4):
        """åœ¨æºåè®®ä¸Šé¢„è®­ç»ƒ"""
        print(f"\né˜¶æ®µ1ï¼šåœ¨æºåè®®ä¸Šé¢„è®­ç»ƒ...")
        print(f"æºåè®®: {list(source_loaders.keys())}")
        self.current_stage = 'source_pretraining'

        # ä¸ºæ¯ä¸ªæºåè®®åˆ›å»ºä¼˜åŒ–å™¨
        all_params = []
        all_params.extend(self.model.protocol_agnostic_encoder.parameters())

        for protocol_name in source_loaders.keys():
            if protocol_name in self.model.protocol_heads:
                all_params.extend(self.model.protocol_heads[protocol_name].parameters())

        all_params.extend(self.model.protocol_classifier.parameters())

        optimizer = optim.AdamW(all_params, lr=lr, weight_decay=0.01)
        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)

        best_avg_f1 = 0.0
        best_boundary_f1 = 0.0
        best_field_perfection = 0.0

        for epoch in range(epochs):
            self.model.train()
            total_loss = 0.0
            batch_count = 0

            # è½®æµè®­ç»ƒæ¯ä¸ªæºåè®®
            for protocol_name, data_loader in source_loaders.items():
                for batch in data_loader:
                    batch = {k: v.to(self.device) if torch.is_tensor(v) else v
                             for k, v in batch.items()}

                    optimizer.zero_grad()

                    outputs = self.model(batch['sequence'], protocol=protocol_name)

                    # è®¡ç®—æŸå¤±
                    loss = self._compute_loss(outputs, batch, protocol_name)

                    loss.backward()
                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
                    optimizer.step()

                    total_loss += loss.item()
                    batch_count += 1

            scheduler.step()

            # éªŒè¯
            if epoch % 5 == 0:
                eval_results = self._evaluate_multiple_protocols(source_loaders)
                avg_metrics = eval_results['averages']

                avg_f1 = avg_metrics['avg_overall_f1']
                avg_boundary_f1 = avg_metrics['avg_boundary_f1']
                avg_field_perfection = avg_metrics['avg_field_perfection']

                if avg_f1 > best_avg_f1:
                    best_avg_f1 = avg_f1
                    best_boundary_f1 = avg_boundary_f1
                    best_field_perfection = avg_field_perfection
                    self._save_checkpoint(f'source_pretrained_multi.pth', epoch, best_avg_f1)

                print(f'  Epoch {epoch}/{epochs}: Loss={total_loss / batch_count:.4f}')
                print(f'    Overall F1={avg_f1:.4f}, Boundary accuracy={avg_metrics["avg_boundary_acc"]:.4f}')
                print(f'    Boundary F1-score={avg_boundary_f1:.4f}, Field perfection={avg_field_perfection:.4f}')

        print(f"æºåè®®é¢„è®­ç»ƒå®Œæˆ:")
        print(f"  - æœ€ä½³Overall F1: {best_avg_f1:.4f}")
        print(f"  - æœ€ä½³Boundary F1-score: {best_boundary_f1:.4f}")
        print(f"  - æœ€ä½³Field perfection: {best_field_perfection:.4f}")

        return best_avg_f1

    def _get_protocol_ids(self, protocol_names):
        """è·å–åè®®ID"""
        ids = []
        for protocol_name in protocol_names:
            if protocol_name in self.protocol_names:
                ids.append(self.protocol_names.index(protocol_name))
            else:
                ids.append(len(self.protocol_names))  # unknown protocol
        return torch.tensor(ids, dtype=torch.long, device=self.device)

    def _evaluate_multiple_protocols(self, data_loaders: Dict[str, DataLoader]) -> Dict:
        """è¯„ä¼°å¤šä¸ªåè®®çš„å¹³å‡æ€§èƒ½"""
        protocol_results = {}

        for protocol_name, data_loader in data_loaders.items():
            metrics = self._evaluate_on_protocol(data_loader, protocol_name)
            protocol_results[protocol_name] = metrics

        # è®¡ç®—å¹³å‡æŒ‡æ ‡
        avg_metrics = {
            'avg_type_f1': np.mean([m['type_f1'] for m in protocol_results.values()]),
            'avg_func_f1': np.mean([m['func_f1'] for m in protocol_results.values()]),
            'avg_boundary_acc': np.mean([m['boundary_acc'] for m in protocol_results.values()]),
            'avg_boundary_precision': np.mean([m['boundary_precision'] for m in protocol_results.values()]),
            'avg_field_perfection': np.mean([m['field_perfection'] for m in protocol_results.values()]),
            'avg_boundary_f1': np.mean([m['boundary_f1'] for m in protocol_results.values()]),
            'avg_overall_f1': np.mean([(m['type_f1'] + m['func_f1']) / 2 for m in protocol_results.values()])
        }

        return {
            'individual_results': protocol_results,
            'averages': avg_metrics
        }

    def _save_checkpoint(self, path: str, epoch: int, best_f1: float):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        try:
            torch.save({
                'epoch': epoch,
                'model_state_dict': self.model.state_dict(),
                'best_f1': best_f1,
                'stage': self.current_stage,
                'protocol_names': self.protocol_names,
                'training_history': dict(self.training_history)
            }, path)
        except Exception as e:
            print(f"ä¿å­˜æ¨¡å‹å¤±è´¥: {e}")


# ===========================
# ä¸»å®éªŒè¿è¡Œå‡½æ•° - ä¿æŒä¸å˜
# ===========================

def run_flexible_transfer_experiment(source_protocols: List[str],
                                     target_protocol: str,
                                     data_root: str = "../Msg2",
                                     model_params: Dict = None,
                                     training_params: Dict = None):
    """è¿è¡Œçµæ´»çš„è·¨åè®®è¿ç§»å­¦ä¹ å®éªŒ"""

    print("=" * 80)
    print("è·¨åè®®è¿ç§»å­¦ä¹ å®éªŒ")
    print("=" * 80)
    print(f"æºåè®®: {source_protocols}")
    print(f"ç›®æ ‡åè®®: {target_protocol}")
    print(f"æ•°æ®æ ¹ç›®å½•: {data_root}")

    # è®¾ç½®é»˜è®¤å‚æ•°
    model_params = model_params or {
        'd_model': 512,
        'encoder_layers': 6
    }

    training_params = training_params or {
        'batch_size': 32,
        'source_epochs': 25,
        'transfer_epochs': 15,
        'finetune_epochs': 15
    }

    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(42)
    np.random.seed(42)
    random.seed(42)

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")

    # 1. åˆå§‹åŒ–æ•°æ®åŠ è½½å™¨
    print("\n1. åˆå§‹åŒ–æ•°æ®åŠ è½½å™¨...")
    data_loader = AdvancedProtocolDataLoader(data_root)

    available_protocols = data_loader.get_available_protocols()
    print(f"å¯ç”¨åè®®: {available_protocols}")

    # éªŒè¯åè®®å¯ç”¨æ€§
    missing_protocols = []
    for protocol in source_protocols + [target_protocol]:
        if protocol not in available_protocols:
            missing_protocols.append(protocol)

    if missing_protocols:
        print(f"ç¼ºå¤±åè®®æ•°æ®: {missing_protocols}")
        print(f"è¯·æ£€æŸ¥ {data_root} ç›®å½•ä¸‹çš„æ•°æ®æ–‡ä»¶")
        return None

    # 2. åŠ è½½æ‰€æœ‰åè®®æ•°æ®
    print("\n2. åŠ è½½åè®®æ•°æ®...")
    all_protocols = set(source_protocols + [target_protocol])
    all_data = {}

    for protocol_name in all_protocols:
        try:
            protocol_data = data_loader.load_protocol_data(protocol_name)
            if len(protocol_data) == 0:
                print(f"  {protocol_name}: æ— æœ‰æ•ˆæ•°æ®")
                continue
            all_data[protocol_name] = protocol_data
            print(f"{protocol_name}: {len(protocol_data)} æ¡æ•°æ®")
        except Exception as e:
            print(f"åŠ è½½ {protocol_name} å¤±è´¥: {e}")
            return None

    if len(all_data) == 0:
        print("æ²¡æœ‰æˆåŠŸåŠ è½½ä»»ä½•åè®®æ•°æ®")
        return None

    # 3. æ•°æ®åˆ†å‰²
    print("\n3. æ•°æ®åˆ†å‰²...")
    data_splits = {}

    for protocol_name, data in all_data.items():
        random.shuffle(data)

        if protocol_name == target_protocol:
            # ç›®æ ‡åè®®ï¼šå°‘æ ·æœ¬å­¦ä¹ 
            train_size = min(200, int(len(data) * 0.6))
            val_size = int(len(data) * 0.2)

            data_splits[protocol_name] = {
                'train': data[:train_size],
                'val': data[train_size:train_size + val_size],
                'test': data[train_size + val_size:]
            }
            print(
                f"  {protocol_name} (ç›®æ ‡): è®­ç»ƒ={train_size}, éªŒè¯={val_size}, æµ‹è¯•={len(data_splits[protocol_name]['test'])}")
        else:
            # æºåè®®ï¼šå……è¶³æ•°æ®
            train_size = int(len(data) * 0.8)
            val_size = int(len(data) * 0.1)

            data_splits[protocol_name] = {
                'train': data[:train_size],
                'val': data[train_size:train_size + val_size],
                'test': data[train_size + val_size:]
            }
            print(
                f"  {protocol_name} (æº): è®­ç»ƒ={train_size}, éªŒè¯={val_size}, æµ‹è¯•={len(data_splits[protocol_name]['test'])}")

    # 4. åˆ›å»ºæ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨
    print("\n4. åˆ›å»ºæ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨...")
    batch_size = training_params.get('batch_size', 32)

    # æºåè®®æ•°æ®åŠ è½½å™¨
    source_loaders = {}
    for protocol_name in source_protocols:
        if protocol_name in data_splits and len(data_splits[protocol_name]['train']) > 0:
            dataset = GenericTransferLearningDataset(
                data_splits[protocol_name]['train'],
                protocol_filter=protocol_name,
                augment=True,
                unified_semantic_types=data_loader.unified_semantic_types,
                unified_semantic_functions=data_loader.unified_semantic_functions
            )
            source_loaders[protocol_name] = DataLoader(
                dataset, batch_size=batch_size, shuffle=True, num_workers=0
            )

    if len(source_loaders) == 0:
        print("æ²¡æœ‰æœ‰æ•ˆçš„æºåè®®æ•°æ®")
        return None

    # ç›®æ ‡åè®®æ•°æ®åŠ è½½å™¨
    if target_protocol not in data_splits:
        print(f"ç›®æ ‡åè®® {target_protocol} æ•°æ®ä¸è¶³")
        return None

    target_train_dataset = GenericTransferLearningDataset(
        data_splits[target_protocol]['train'],
        protocol_filter=target_protocol,
        augment=True,
        unified_semantic_types=data_loader.unified_semantic_types,
        unified_semantic_functions=data_loader.unified_semantic_functions
    )
    target_val_dataset = GenericTransferLearningDataset(
        data_splits[target_protocol]['val'],
        protocol_filter=target_protocol,
        unified_semantic_types=data_loader.unified_semantic_types,
        unified_semantic_functions=data_loader.unified_semantic_functions
    )
    target_test_dataset = GenericTransferLearningDataset(
        data_splits[target_protocol]['test'],
        protocol_filter=target_protocol,
        unified_semantic_types=data_loader.unified_semantic_types,
        unified_semantic_functions=data_loader.unified_semantic_functions
    )

    target_train_loader = DataLoader(target_train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)
    target_val_loader = DataLoader(target_val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)
    target_test_loader = DataLoader(target_test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)

    # 5. åˆ›å»ºæ¨¡å‹
    print("\nâš¡ 5. åˆ›å»ºé€šç”¨è·¨åè®®è¿ç§»å­¦ä¹ æ¨¡å‹...")
    all_protocol_names = list(all_protocols)

    model = GenericCrossProtocolTransferModel(
        protocol_names=all_protocol_names,
        d_model=model_params.get('d_model', 512),
        encoder_layers=model_params.get('encoder_layers', 6),
        num_semantic_types=len(data_loader.unified_semantic_types),
        num_semantic_functions=len(data_loader.unified_semantic_functions)
    )

    total_params = sum(p.numel() for p in model.parameters())
    print(f"æ¨¡å‹æ€»å‚æ•°é‡: {total_params:,}")
    print(f"æ”¯æŒåè®®: {all_protocol_names}")

    # 6. å¼€å§‹è¿ç§»å­¦ä¹ å®éªŒ
    print("\n6. å¼€å§‹è¿ç§»å­¦ä¹ å®éªŒ...")
    trainer = GenericTransferLearningTrainer(model, device, all_protocol_names)

    # åŸºçº¿å®éªŒï¼šç›´æ¥åœ¨ç›®æ ‡åè®®ä¸Šè®­ç»ƒ
    print(f"\nåŸºçº¿å®éªŒï¼šç›´æ¥åœ¨ {target_protocol.upper()} ä¸Šè®­ç»ƒ...")
    baseline_model = GenericCrossProtocolTransferModel(
        protocol_names=[target_protocol],
        d_model=model_params.get('d_model', 512),
        encoder_layers=model_params.get('encoder_layers', 6),
        num_semantic_types=len(data_loader.unified_semantic_types),
        num_semantic_functions=len(data_loader.unified_semantic_functions)
    ).to(device)

    baseline_trainer = GenericTransferLearningTrainer(baseline_model, device, [target_protocol])
    baseline_f1 = baseline_trainer.transfer_to_target(
        target_train_loader, target_val_loader,
        target_protocol, epochs=20, freeze_encoder=False
    )

    print(f"åŸºçº¿ç»“æœï¼ˆæ— è¿ç§»å­¦ä¹ ï¼‰: F1 = {baseline_f1:.4f}")

    # è¿ç§»å­¦ä¹ å®éªŒ
    print(f"\nå¼€å§‹å®Œæ•´è¿ç§»å­¦ä¹ æµç¨‹...")

    # é˜¶æ®µ1ï¼šæºåè®®é¢„è®­ç»ƒ
    source_f1 = trainer.train_source_protocols(
        source_loaders,
        epochs=training_params.get('source_epochs', 25)
    )

    # é˜¶æ®µ2aï¼šå†»ç»“ç¼–ç å™¨è¿ç§»
    frozen_f1 = trainer.transfer_to_target(
        target_train_loader, target_val_loader, target_protocol,
        epochs=training_params.get('transfer_epochs', 15),
        freeze_encoder=True
    )

    # é˜¶æ®µ2bï¼šç«¯åˆ°ç«¯å¾®è°ƒ
    finetuned_f1 = trainer.transfer_to_target(
        target_train_loader, target_val_loader, target_protocol,
        epochs=training_params.get('finetune_epochs', 15),
        freeze_encoder=False
    )

    # 7. æœ€ç»ˆæµ‹è¯•è¯„ä¼°
    print("\n7. æœ€ç»ˆæµ‹è¯•è¯„ä¼°...")

    # åŸºçº¿æ¨¡å‹æµ‹è¯•
    baseline_test_metrics = baseline_trainer._evaluate_on_protocol(target_test_loader, target_protocol)
    baseline_test_f1 = (baseline_test_metrics['type_f1'] + baseline_test_metrics['func_f1']) / 2

    # è¿ç§»å­¦ä¹ æ¨¡å‹æµ‹è¯•
    transfer_test_metrics = trainer._evaluate_on_protocol(target_test_loader, target_protocol)
    transfer_test_f1 = (transfer_test_metrics['type_f1'] + transfer_test_metrics['func_f1']) / 2

    # ç»“æœå¯¹æ¯”
    print(f"\n" + "=" * 80)
    print("è¿ç§»å­¦ä¹ æ•ˆæœå¯¹æ¯”")
    print("=" * 80)
    print(f"åè®®ç»„åˆ: {source_protocols} â†’ {target_protocol}")
    print(f"åŸºçº¿æ¨¡å‹ (æ— è¿ç§»):     {baseline_test_f1:.4f}")
    print(f"è¿ç§»å­¦ä¹ æ¨¡å‹:         {transfer_test_f1:.4f}")
    print(f"ç»å¯¹æå‡:            {transfer_test_f1 - baseline_test_f1:+.4f}")

    if baseline_test_f1 > 0:
        print(f"ç›¸å¯¹æå‡:            {((transfer_test_f1 / baseline_test_f1) - 1) * 100:+.1f}%")

    # ã€æ–°å¢ã€‘Format ExtractionæŒ‡æ ‡å¯¹æ¯”
    print(f"\nFormat ExtractionæŒ‡æ ‡å¯¹æ¯”:")
    print(f"åŸºçº¿æ¨¡å‹:")
    print(f"  - accuracy: {baseline_test_metrics['boundary_acc']:.4f}")
    print(f"  - F1-score: {baseline_test_metrics['boundary_f1']:.4f}")
    print(f"  - perfection: {baseline_test_metrics['field_perfection']:.4f}")
    print(f"è¿ç§»å­¦ä¹ æ¨¡å‹:")
    print(f"  - accuracy: {transfer_test_metrics['boundary_acc']:.4f}")
    print(f"  - F1-score: {transfer_test_metrics['boundary_f1']:.4f}")
    print(f"  - perfection: {transfer_test_metrics['field_perfection']:.4f}")

    # è¯¦ç»†åˆ†æ
    improvement = transfer_test_f1 - baseline_test_f1
    perfection_improvement = transfer_test_metrics['field_perfection'] - baseline_test_metrics['field_perfection']
    f1_improvement = transfer_test_metrics['boundary_f1'] - baseline_test_metrics['boundary_f1']

    if improvement > 0.1:
        print(f"\nè¿ç§»å­¦ä¹ å¤§æˆåŠŸï¼")
        print(f"   è·¨åè®®çŸ¥è¯†è¿ç§»æ˜¾è‘—æå‡äº† {target_protocol} åè®®æ€§èƒ½")
    elif improvement > 0.05:
        print(f"\nè¿ç§»å­¦ä¹ æœ‰æ•ˆï¼")
        print(f"   æ¨¡å‹æˆåŠŸå­¦åˆ°äº†è·¨åè®®çš„é€šç”¨ç‰¹å¾")
    elif improvement > 0:
        print(f"\nè¿ç§»å­¦ä¹ æœ‰è½»å¾®æå‡")
    else:
        print(f"\nè¿ç§»å­¦ä¹ æ•ˆæœä¸æ˜æ˜¾ï¼Œå¯èƒ½éœ€è¦è°ƒæ•´ç­–ç•¥")

    if perfection_improvement > 0.1:
        print(f"   å­—æ®µå®Œç¾åŒ¹é…ç‡æ˜¾è‘—æå‡: {perfection_improvement:+.4f}")

    if f1_improvement > 0.1:
        print(f"   è¾¹ç•Œæ£€æµ‹F1åˆ†æ•°æ˜¾è‘—æå‡: {f1_improvement:+.4f}")

    return {
        'baseline_f1': baseline_test_f1,
        'transfer_f1': transfer_test_f1,
        'improvement': improvement,
        'source_protocols': source_protocols,
        'target_protocol': target_protocol,
        'baseline_metrics': baseline_test_metrics,
        'transfer_metrics': transfer_test_metrics,
        'perfection_improvement': perfection_improvement,
        'f1_improvement': f1_improvement
    }


# ===========================
# å‘½ä»¤è¡Œæ¥å£ - ä¿æŒä¸å˜
# ===========================

def main():
    parser = argparse.ArgumentParser(description='è·¨åè®®è¿ç§»å­¦ä¹ å®éªŒ')

    parser.add_argument('--source-protocols', nargs='+',
                        default=['dnp3'],
                        help='æºåè®®åˆ—è¡¨ (default: dnp3)')

    parser.add_argument('--target-protocol', type=str, default='modbus',
                        help='ç›®æ ‡åè®® (default: modbus)')

    parser.add_argument('--data-root', type=str, default='../Msg2',
                        help='æ•°æ®æ ¹ç›®å½• (default: ../Msg2)')

    parser.add_argument('--d-model', type=int, default=256,
                        help='æ¨¡å‹ç»´åº¦ (default: 256)')

    parser.add_argument('--encoder-layers', type=int, default=4,
                        help='ç¼–ç å™¨å±‚æ•° (default: 4)')

    parser.add_argument('--batch-size', type=int, default=16,
                        help='æ‰¹æ¬¡å¤§å° (default: 16)')

    parser.add_argument('--source-epochs', type=int, default=10,
                        help='æºåè®®è®­ç»ƒè½®æ•° (default: 10)')

    parser.add_argument('--transfer-epochs', type=int, default=8,
                        help='è¿ç§»è®­ç»ƒè½®æ•° (default: 8)')

    parser.add_argument('--finetune-epochs', type=int, default=8,
                        help='å¾®è°ƒè®­ç»ƒè½®æ•° (default: 8)')

    args = parser.parse_args()

    # è®¾ç½®å‚æ•°
    model_params = {
        'd_model': args.d_model,
        'encoder_layers': args.encoder_layers
    }

    training_params = {
        'batch_size': args.batch_size,
        'source_epochs': args.source_epochs,
        'transfer_epochs': args.transfer_epochs,
        'finetune_epochs': args.finetune_epochs
    }

    # è¿è¡Œå®éªŒ
    results = run_flexible_transfer_experiment(
        source_protocols=args.source_protocols,
        target_protocol=args.target_protocol,
        data_root=args.data_root,
        model_params=model_params,
        training_params=training_params
    )

    if results:
        print(f"\nå®éªŒå®Œæˆï¼")
        print(
            f"ä» {results['source_protocols']} åˆ° {results['target_protocol']} çš„è¿ç§»æ•ˆæœ: {results['improvement']:+.4f}")
        print(f"å­—æ®µå®Œç¾åŒ¹é…ç‡æå‡: {results['perfection_improvement']:+.4f}")
        print(f"è¾¹ç•Œæ£€æµ‹F1åˆ†æ•°æå‡: {results['f1_improvement']:+.4f}")
    else:
        print(f"\nå®éªŒå¤±è´¥ï¼Œè¯·æ£€æŸ¥æ•°æ®å’Œé…ç½®")


if __name__ == "__main__":
    # æµ‹è¯•è¿è¡Œ
    results = run_flexible_transfer_experiment(['modbus'], 'dns')