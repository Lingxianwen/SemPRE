#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ä¿®å¤ç‰ˆFew-Shot Learningè·¨åè®®è¿ç§»å­¦ä¹ å®éªŒ

ä¸»è¦ä¿®å¤ï¼š
1. ä¿®å¤Prototypical Networkçš„æ ¸å¿ƒé—®é¢˜
2. æ”¹è¿›æ•°æ®é‡‡æ ·å’Œæ ‡ç­¾ç”Ÿæˆ
3. ä¼˜åŒ–æŸå¤±å‡½æ•°å’Œè®­ç»ƒç­–ç•¥
4. å¢å¼ºå®éªŒç¨³å®šæ€§å’Œé”™è¯¯å¤„ç†
"""

import sys
import os
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Optional, Union
import random
import time
from datetime import datetime
import json
from collections import defaultdict, Counter
import matplotlib.pyplot as plt
import seaborn as sns
from torch.utils.data import DataLoader, Dataset, Sampler
import argparse
from sklearn.metrics import f1_score, accuracy_score, precision_recall_fscore_support
import warnings

warnings.filterwarnings('ignore')

# å¯¼å…¥ç°æœ‰æ¨¡å‹ç±»
try:
    from Model_717 import (
        AdvancedProtocolDataLoader,
        GenericTransferLearningDataset,
        GenericCrossProtocolTransferModel,
        GenericTransferLearningTrainer
    )

    print("âœ… æˆåŠŸå¯¼å…¥åŸå§‹æ¨¡å‹ç±»")
except ImportError as e:
    print(f"âŒ å¯¼å…¥åŸå§‹æ¨¡å‹å¤±è´¥: {e}")
    sys.exit(1)


class FixedPrototypicalNetwork(nn.Module):
    """ä¿®å¤ç‰ˆåŸå‹ç½‘ç»œå®ç° - è§£å†³æ ¸å¿ƒé—®é¢˜"""

    def __init__(self, base_model: GenericCrossProtocolTransferModel,
                 embedding_dim: int = 256, temperature: float = 1.0):
        super().__init__()
        self.base_model = base_model
        self.embedding_dim = embedding_dim
        self.temperature = temperature

        # ã€ä¿®å¤1ã€‘ä¸å†»ç»“è¿‡å¤šå‚æ•°ï¼Œä¿æŒæ¨¡å‹å¯è®­ç»ƒæ€§
        frozen_params = 0
        for name, param in self.base_model.named_parameters():
            if 'protocol_agnostic_encoder.transformer_encoder' in name:
                # åªå†»ç»“éƒ¨åˆ†transformerå±‚
                layer_num = self._extract_layer_number(name)
                if layer_num is not None and layer_num < 2:  # åªå†»ç»“å‰2å±‚
                    param.requires_grad = False
                    frozen_params += 1

        print(f"ğŸ”§ åŸå‹ç½‘ç»œ: å†»ç»“äº†{frozen_params}ä¸ªå‚æ•°")

        # ã€ä¿®å¤2ã€‘ç®€åŒ–ç‰¹å¾æå–å™¨ï¼Œé¿å…è¿‡åº¦å¤æ‚
        self.feature_extractor = nn.Sequential(
            nn.Linear(embedding_dim, embedding_dim // 2),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(embedding_dim // 2, embedding_dim // 4)
        )

        # å½’ä¸€åŒ–å±‚
        self.norm = nn.LayerNorm(embedding_dim // 4)

        # ã€æ–°å¢ã€‘ç±»åˆ«æƒé‡å­¦ä¹ 
        self.class_weights = nn.Parameter(torch.ones(2))

    def _extract_layer_number(self, name: str) -> Optional[int]:
        """ä»å‚æ•°åä¸­æå–å±‚å·"""
        try:
            if 'layers.' in name:
                parts = name.split('layers.')[1].split('.')[0]
                return int(parts)
        except:
            pass
        return None

    def extract_features(self, x: torch.Tensor, protocol: str = None) -> torch.Tensor:
        """æå–åºåˆ—ç‰¹å¾ - ä¿®å¤ç‰ˆæœ¬"""
        try:
            with torch.no_grad():
                # ã€ä¿®å¤3ã€‘ä½¿ç”¨æ›´ç¨³å®šçš„ç‰¹å¾æå–æ–¹å¼
                encoder_outputs = self.base_model.protocol_agnostic_encoder(x)
                features = encoder_outputs['protocol_agnostic_features']

            # ã€ä¿®å¤4ã€‘æ”¹è¿›æ± åŒ–ç­–ç•¥
            # ä½¿ç”¨æœ€å¤§æ± åŒ–å’Œå¹³å‡æ± åŒ–çš„ç»„åˆ
            max_pooled = torch.max(features, dim=1)[0]  # [batch, dim]
            avg_pooled = torch.mean(features, dim=1)  # [batch, dim]

            # ç»„åˆç‰¹å¾
            combined_features = (max_pooled + avg_pooled) / 2

            # ç‰¹å¾æŠ•å½±
            projected_features = self.feature_extractor(combined_features)
            normalized_features = self.norm(projected_features)

            return normalized_features

        except Exception as e:
            print(f"âŒ ç‰¹å¾æå–å¤±è´¥: {e}")
            # è¿”å›éšæœºç‰¹å¾ä½œä¸ºå¤‡é€‰
            batch_size = x.size(0)
            return torch.randn(batch_size, self.embedding_dim // 4, device=x.device) * 0.01

    def compute_prototypes(self, support_features: torch.Tensor,
                           support_labels: torch.Tensor) -> torch.Tensor:
        """è®¡ç®—ç±»åˆ«åŸå‹ - ä¿®å¤ç‰ˆæœ¬"""
        unique_labels = torch.unique(support_labels)
        prototypes = []

        for class_id in unique_labels:
            class_mask = (support_labels == class_id)
            class_features = support_features[class_mask]

            if len(class_features) > 0:
                # ã€ä¿®å¤5ã€‘ä½¿ç”¨æ›´ç¨³å®šçš„åŸå‹è®¡ç®—
                if len(class_features) > 1:
                    # å¦‚æœæœ‰å¤šä¸ªæ ·æœ¬ï¼Œè®¡ç®—åŠ æƒå¹³å‡
                    weights = torch.softmax(torch.norm(class_features, dim=1), dim=0)
                    prototype = torch.sum(class_features * weights.unsqueeze(1), dim=0)
                else:
                    # åªæœ‰ä¸€ä¸ªæ ·æœ¬æ—¶ç›´æ¥ä½¿ç”¨
                    prototype = class_features[0]
            else:
                # ã€ä¿®å¤6ã€‘æ›´å¥½çš„é»˜è®¤åŸå‹
                prototype = torch.zeros(support_features.size(1), device=support_features.device)

            prototypes.append(prototype)

        if len(prototypes) == 0:
            # åº”æ€¥æƒ…å†µï¼šåˆ›å»ºé»˜è®¤åŸå‹
            prototypes = [torch.zeros(support_features.size(1), device=support_features.device) for _ in range(2)]

        prototypes = torch.stack(prototypes)  # [n_classes, feature_dim]

        # åŸå‹å½’ä¸€åŒ–
        prototypes = F.normalize(prototypes, p=2, dim=1)

        return prototypes

    def classify_queries(self, query_features: torch.Tensor,
                         prototypes: torch.Tensor) -> torch.Tensor:
        """åŸºäºåŸå‹å¯¹æŸ¥è¯¢æ ·æœ¬åˆ†ç±» - ä¿®å¤ç‰ˆæœ¬"""
        # æŸ¥è¯¢ç‰¹å¾å½’ä¸€åŒ–
        query_features = F.normalize(query_features, p=2, dim=1)

        # ã€ä¿®å¤7ã€‘ç¡®ä¿åŸå‹å’ŒæŸ¥è¯¢ç‰¹å¾ç»´åº¦åŒ¹é…
        if prototypes.size(0) == 0:
            # åº”æ€¥æƒ…å†µï¼šåˆ›å»ºé»˜è®¤logits
            return torch.zeros(query_features.size(0), 2, device=query_features.device)

        # è®¡ç®—è·ç¦»ï¼ˆä½¿ç”¨æ¬§å‡ é‡Œå¾—è·ç¦»è€Œä¸æ˜¯ä½™å¼¦ç›¸ä¼¼åº¦ï¼Œæ›´ç¨³å®šï¼‰
        distances = torch.cdist(query_features.unsqueeze(0), prototypes.unsqueeze(0)).squeeze(0)

        # è½¬æ¢ä¸ºç›¸ä¼¼åº¦ï¼ˆè·ç¦»è¶Šå°ï¼Œç›¸ä¼¼åº¦è¶Šé«˜ï¼‰
        similarities = -distances  # [n_query, n_classes]

        # ç¡®ä¿è¾“å‡ºç»´åº¦æ­£ç¡®
        if similarities.size(1) < 2:
            # å¦‚æœç±»åˆ«æ•°ä¸è¶³ï¼Œè¡¥å……
            padding = torch.zeros(similarities.size(0), 2 - similarities.size(1), device=similarities.device)
            similarities = torch.cat([similarities, padding], dim=1)
        elif similarities.size(1) > 2:
            # å¦‚æœç±»åˆ«æ•°è¿‡å¤šï¼Œåªå–å‰2ä¸ª
            similarities = similarities[:, :2]

        # åº”ç”¨æ¸©åº¦ç¼©æ”¾å’Œç±»åˆ«æƒé‡
        logits = similarities / self.temperature * self.class_weights.unsqueeze(0)

        return logits

    def forward(self, support_data: Dict, query_data: Dict) -> Dict:
        """å‰å‘ä¼ æ’­è¿›è¡ŒFew-Shotåˆ†ç±» - ä¿®å¤ç‰ˆæœ¬"""
        try:
            # æå–supportå’Œqueryç‰¹å¾
            support_features = self.extract_features(support_data['sequence'])
            query_features = self.extract_features(query_data['sequence'])

            # ã€ä¿®å¤8ã€‘ç¡®ä¿æ ‡ç­¾æ ¼å¼æ­£ç¡®
            support_labels = support_data['labels']
            if support_labels.dim() > 1:
                support_labels = support_labels.view(-1)

            # è®¡ç®—åŸå‹
            prototypes = self.compute_prototypes(support_features, support_labels)

            # åˆ†ç±»æŸ¥è¯¢æ ·æœ¬
            logits = self.classify_queries(query_features, prototypes)

            return {
                'logits': logits,
                'support_features': support_features,
                'query_features': query_features,
                'prototypes': prototypes
            }

        except Exception as e:
            print(f"âŒ åŸå‹ç½‘ç»œå‰å‘ä¼ æ’­å¤±è´¥: {e}")
            # è¿”å›é»˜è®¤è¾“å‡º
            batch_size = query_data['sequence'].size(0)
            return {
                'logits': torch.zeros(batch_size, 2, device=query_data['sequence'].device),
                'support_features': torch.zeros(1, self.embedding_dim // 4, device=query_data['sequence'].device),
                'query_features': torch.zeros(batch_size, self.embedding_dim // 4,
                                              device=query_data['sequence'].device),
                'prototypes': torch.zeros(2, self.embedding_dim // 4, device=query_data['sequence'].device)
            }


class ImprovedFewShotDataSampler:
    """æ”¹è¿›çš„Few-Shotæ•°æ®é‡‡æ ·å™¨"""

    def __init__(self, dataset: GenericTransferLearningDataset, target_protocol: str):
        self.dataset = dataset
        self.target_protocol = target_protocol

        # ã€æ”¹è¿›1ã€‘åˆ†ææ•°æ®è´¨é‡
        self.valid_indices = []
        self.protocol_indices = defaultdict(list)

        for idx in range(len(dataset)):
            try:
                sample = dataset[idx]
                if (sample['valid_boundaries_count'] > 0 and
                        sample['actual_length'] > 8):  # ç¡®ä¿æ•°æ®è´¨é‡
                    self.valid_indices.append(idx)
                    self.protocol_indices[sample['protocol']].append(idx)
            except:
                continue

        print(f"ğŸ”„ é‡‡æ ·å™¨åˆå§‹åŒ–: {len(self.valid_indices)}/{len(dataset)} æœ‰æ•ˆæ ·æœ¬")
        print(f"   åè®®åˆ†å¸ƒ: {dict(Counter([dataset[i]['protocol'] for i in self.valid_indices]))}")

    def sample_episode(self, k_shot: int, n_query: int = 15) -> Tuple[List[int], List[int]]:
        """é‡‡æ ·å•ä¸ªepisodeçš„supportå’Œqueryé›† - æ”¹è¿›ç‰ˆæœ¬"""
        if len(self.valid_indices) < k_shot + n_query:
            # ã€æ”¹è¿›2ã€‘æ•°æ®ä¸è¶³æ—¶çš„å¤„ç†ç­–ç•¥
            all_indices = self.valid_indices * ((k_shot + n_query) // len(self.valid_indices) + 2)
            random.shuffle(all_indices)
        else:
            all_indices = self.valid_indices.copy()
            random.shuffle(all_indices)

        # ã€æ”¹è¿›3ã€‘ç¡®ä¿supportå’Œqueryçš„å¤šæ ·æ€§
        support_indices = []
        query_indices = []

        # é¦–å…ˆå°è¯•ä»æ¯ä¸ªåè®®ä¸­é‡‡æ ·
        protocols = list(self.protocol_indices.keys())
        if len(protocols) > 1:
            samples_per_protocol = max(1, k_shot // len(protocols))
            for protocol in protocols:
                if len(support_indices) >= k_shot:
                    break
                protocol_samples = self.protocol_indices[protocol].copy()
                random.shuffle(protocol_samples)
                support_indices.extend(protocol_samples[:samples_per_protocol])

        # è¡¥å……åˆ°k_shot
        remaining_indices = [idx for idx in all_indices if idx not in support_indices]
        support_indices.extend(remaining_indices[:k_shot - len(support_indices)])
        support_indices = support_indices[:k_shot]

        # é‡‡æ ·query
        query_candidates = [idx for idx in all_indices if idx not in support_indices]
        query_indices = query_candidates[:n_query]

        return support_indices, query_indices

    def create_balanced_labels(self, data_batch: List) -> torch.Tensor:
        """åˆ›å»ºå¹³è¡¡çš„äºŒåˆ†ç±»æ ‡ç­¾"""
        labels = []
        target_count = 0

        for item in data_batch:
            if item['protocol'] == self.target_protocol:
                labels.append(1)
                target_count += 1
            else:
                labels.append(0)

        # ã€æ”¹è¿›4ã€‘ç¡®ä¿æ ‡ç­¾å¹³è¡¡
        if target_count == 0 or target_count == len(data_batch):
            # å¦‚æœæ ‡ç­¾å®Œå…¨ä¸å¹³è¡¡ï¼Œéšæœºåˆ†é…ä¸€äº›
            for i in range(len(labels) // 2):
                labels[i] = 1 - labels[i]

        return torch.tensor(labels, dtype=torch.long)


class FixedFewShotLearningExperiment:
    """ä¿®å¤ç‰ˆFew-Shot Learningå®éªŒç±»"""

    def __init__(self, data_root: str = "../Msg2", device: str = None):
        self.data_root = data_root
        self.device = device if device else ('cuda' if torch.cuda.is_available() else 'cpu')

        print(f"ğŸš€ åˆå§‹åŒ–ä¿®å¤ç‰ˆFew-Shot Learningå®éªŒ...")
        print(f"æ•°æ®æ ¹ç›®å½•: {data_root}")
        print(f"è®¡ç®—è®¾å¤‡: {self.device}")

        # åˆå§‹åŒ–æ•°æ®åŠ è½½å™¨
        self.data_loader = AdvancedProtocolDataLoader(data_root)
        self.available_protocols = self.data_loader.get_available_protocols()

        # åŠ è½½æ‰€æœ‰åè®®æ•°æ®
        self.all_data = {}
        self.load_all_protocol_data()

        print(f"å¯ç”¨åè®®: {self.available_protocols}")
        print(f"å·²åŠ è½½åè®®: {list(self.all_data.keys())}")

    def load_all_protocol_data(self):
        """åŠ è½½æ‰€æœ‰åè®®æ•°æ®"""
        print("\nğŸ“Š åŠ è½½åè®®æ•°æ®...")
        for protocol_name in self.available_protocols:
            try:
                protocol_data = self.data_loader.load_protocol_data(protocol_name)
                if len(protocol_data) > 0:
                    # ã€æ”¹è¿›1ã€‘è¿‡æ»¤ä½è´¨é‡æ•°æ®
                    filtered_data = []
                    for sample in protocol_data:
                        if (len(sample['ground_truth']['syntax_boundaries']) > 0 and
                                sample['length'] >= 8):
                            filtered_data.append(sample)

                    if len(filtered_data) >= 10:  # è‡³å°‘éœ€è¦10ä¸ªæœ‰æ•ˆæ ·æœ¬
                        self.all_data[protocol_name] = filtered_data
                        print(f"  {protocol_name}: {len(filtered_data)}/{len(protocol_data)} æœ‰æ•ˆæ•°æ®")
                    else:
                        print(f"  {protocol_name}: æ•°æ®è´¨é‡ä¸è¶³ï¼Œè·³è¿‡")

            except Exception as e:
                print(f"  {protocol_name}: åŠ è½½å¤±è´¥ - {e}")

    def run_few_shot_experiment(self, source_protocols: List[str], target_protocol: str,
                                shots: int = 5, episodes: int = 100, method: str = 'prototypical') -> Dict:
        """è¿è¡ŒFew-Shot Learningå®éªŒ - ä¿®å¤ç‰ˆæœ¬"""
        print(f"\nğŸ¯ ä¿®å¤ç‰ˆFew-Shot Learningå®éªŒ: {shots}-shot")
        print(f"æºåè®®: {source_protocols}")
        print(f"ç›®æ ‡åè®®: {target_protocol}")
        print(f"æ–¹æ³•: {method}")

        # æ£€æŸ¥æ•°æ®å¯ç”¨æ€§
        missing_protocols = []
        for protocol in source_protocols + [target_protocol]:
            if protocol not in self.all_data or len(self.all_data[protocol]) < shots + 5:
                missing_protocols.append(protocol)

        if missing_protocols:
            print(f"âŒ ç¼ºå¤±æˆ–æ•°æ®ä¸è¶³çš„åè®®: {missing_protocols}")
            return {'success': False, 'error': 'insufficient_data'}

        try:
            if method == 'prototypical':
                return self._run_fixed_prototypical_experiment(
                    source_protocols, target_protocol, shots, episodes
                )
            else:
                return self._run_improved_simple_experiment(
                    source_protocols, target_protocol, shots, episodes
                )

        except Exception as e:
            print(f"âŒ å®éªŒå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return {'success': False, 'error': str(e)}

    def _run_fixed_prototypical_experiment(self, source_protocols: List[str],
                                           target_protocol: str, shots: int, episodes: int) -> Dict:
        """è¿è¡Œä¿®å¤ç‰ˆåŸå‹ç½‘ç»œFew-Shotå®éªŒ"""
        print(f"\nğŸ”¬ ä¿®å¤ç‰ˆåŸå‹ç½‘ç»œ {shots}-shot å®éªŒ...")

        # å‡†å¤‡æºæ•°æ®
        source_data = []
        for protocol in source_protocols:
            source_data.extend(self.all_data[protocol][:100])  # é™åˆ¶æºæ•°æ®é‡

        target_data = self.all_data[target_protocol][:500]  # é™åˆ¶ç›®æ ‡æ•°æ®é‡

        print(f"æºæ•°æ®: {len(source_data)} æ¡")
        print(f"ç›®æ ‡æ•°æ®: {len(target_data)} æ¡")

        # åˆ›å»ºåŸºç¡€æ¨¡å‹
        base_model = GenericCrossProtocolTransferModel(
            protocol_names=source_protocols + [target_protocol],
            d_model=256,
            encoder_layers=4,
            num_semantic_types=len(self.data_loader.unified_semantic_types),
            num_semantic_functions=len(self.data_loader.unified_semantic_functions)
        ).to(self.device)

        # ã€æ”¹è¿›1ã€‘æ›´å¥½çš„é¢„è®­ç»ƒç­–ç•¥
        if source_data:
            print("  é¢„è®­ç»ƒåŸºç¡€æ¨¡å‹...")
            source_dataset = GenericTransferLearningDataset(
                source_data,
                unified_semantic_types=self.data_loader.unified_semantic_types,
                unified_semantic_functions=self.data_loader.unified_semantic_functions
            )
            self._enhanced_pretrain(base_model, source_dataset, source_protocols)

        # åˆ›å»ºä¿®å¤ç‰ˆåŸå‹ç½‘ç»œ
        proto_net = FixedPrototypicalNetwork(base_model, embedding_dim=256).to(self.device)

        # åˆ›å»ºæ•°æ®é‡‡æ ·å™¨
        target_dataset = GenericTransferLearningDataset(
            target_data,
            protocol_filter=target_protocol,
            unified_semantic_types=self.data_loader.unified_semantic_types,
            unified_semantic_functions=self.data_loader.unified_semantic_functions
        )

        sampler = ImprovedFewShotDataSampler(target_dataset, target_protocol)

        # Few-Shotæµ‹è¯•
        episode_results = []
        proto_net.eval()

        print(f"  å¼€å§‹ {episodes} ä¸ªæµ‹è¯•å›åˆ...")

        for episode in range(episodes):
            try:
                episode_result = self._run_fixed_prototypical_episode(
                    proto_net, target_dataset, sampler, shots, target_protocol
                )
                episode_results.append(episode_result)

                if episode % 20 == 0 and episode_results:
                    recent_results = episode_results[-20:]
                    avg_acc = np.mean([r['accuracy'] for r in recent_results])
                    avg_f1 = np.mean([r['f1_score'] for r in recent_results])
                    print(f"    Episode {episode}/{episodes}: å‡†ç¡®ç‡={avg_acc:.4f}, F1={avg_f1:.4f}")

            except Exception as e:
                print(f"    Episode {episode} å¤±è´¥: {e}")
                episode_results.append({
                    'accuracy': 0.0, 'f1_score': 0.0, 'precision': 0.0, 'recall': 0.0
                })

        # è®¡ç®—æœ€ç»ˆç»“æœ
        if not episode_results:
            return {'success': False, 'error': 'no_valid_episodes'}

        avg_accuracy = np.mean([r['accuracy'] for r in episode_results])
        avg_f1 = np.mean([r['f1_score'] for r in episode_results])
        avg_precision = np.mean([r['precision'] for r in episode_results])
        avg_recall = np.mean([r['recall'] for r in episode_results])

        print(f"âœ… ä¿®å¤ç‰ˆåŸå‹ç½‘ç»œç»“æœ:")
        print(f"  å¹³å‡å‡†ç¡®ç‡: {avg_accuracy:.4f}")
        print(f"  å¹³å‡F1åˆ†æ•°: {avg_f1:.4f}")
        print(f"  å¹³å‡ç²¾ç¡®ç‡: {avg_precision:.4f}")
        print(f"  å¹³å‡å¬å›ç‡: {avg_recall:.4f}")

        return {
            'success': True,
            'avg_accuracy': avg_accuracy,
            'avg_overall_f1': avg_f1,  # ä¸»è¦æŒ‡æ ‡
            'avg_precision': avg_precision,
            'avg_recall': avg_recall,
            'std_accuracy': np.std([r['accuracy'] for r in episode_results]),
            'std_f1': np.std([r['f1_score'] for r in episode_results]),
            'episode_results': episode_results
        }

    def _run_fixed_prototypical_episode(self, proto_net: FixedPrototypicalNetwork,
                                        dataset: GenericTransferLearningDataset,
                                        sampler: ImprovedFewShotDataSampler,
                                        shots: int, target_protocol: str) -> Dict:
        """è¿è¡Œå•ä¸ªä¿®å¤ç‰ˆåŸå‹ç½‘ç»œæµ‹è¯•å›åˆ"""
        try:
            # é‡‡æ ·æ•°æ®
            support_indices, query_indices = sampler.sample_episode(shots, n_query=15)

            if len(support_indices) == 0 or len(query_indices) == 0:
                return {'accuracy': 0.0, 'f1_score': 0.0, 'precision': 0.0, 'recall': 0.0}

            # å‡†å¤‡æ•°æ®
            support_data = [dataset[idx] for idx in support_indices]
            query_data = [dataset[idx] for idx in query_indices]

            support_batch = self._collate_batch(support_data)
            query_batch = self._collate_batch(query_data)

            # åˆ›å»ºæ ‡ç­¾
            support_labels = sampler.create_balanced_labels(support_data)
            query_labels = sampler.create_balanced_labels(query_data)

            with torch.no_grad():
                # å‡†å¤‡è¾“å…¥
                support_input = {
                    'sequence': support_batch['sequence'].to(self.device),
                    'labels': support_labels.to(self.device)
                }

                query_input = {
                    'sequence': query_batch['sequence'].to(self.device)
                }

                # å‰å‘ä¼ æ’­
                outputs = proto_net.forward(support_input, query_input)

                # è·å–é¢„æµ‹ç»“æœ
                predictions = torch.argmax(outputs['logits'], dim=1).cpu().numpy()
                query_labels_np = query_labels.numpy()

                # è®¡ç®—æŒ‡æ ‡
                accuracy = accuracy_score(query_labels_np, predictions)

                # è®¡ç®—F1ç­‰æŒ‡æ ‡ï¼Œå¤„ç†è¾¹ç•Œæƒ…å†µ
                if len(np.unique(query_labels_np)) > 1:
                    f1 = f1_score(query_labels_np, predictions, average='weighted', zero_division=0)
                    precision = \
                    precision_recall_fscore_support(query_labels_np, predictions, average='weighted', zero_division=0)[
                        0]
                    recall = \
                    precision_recall_fscore_support(query_labels_np, predictions, average='weighted', zero_division=0)[
                        1]
                else:
                    f1 = accuracy  # å¦‚æœåªæœ‰ä¸€ä¸ªç±»åˆ«ï¼Œä½¿ç”¨å‡†ç¡®ç‡
                    precision = accuracy
                    recall = accuracy

                return {
                    'accuracy': float(accuracy),
                    'f1_score': float(f1),
                    'precision': float(precision),
                    'recall': float(recall)
                }

        except Exception as e:
            print(f"      Episodeæ‰§è¡Œå¤±è´¥: {e}")
            return {'accuracy': 0.0, 'f1_score': 0.0, 'precision': 0.0, 'recall': 0.0}

    def _enhanced_pretrain(self, model, dataset, protocols, epochs=5):
        """å¢å¼ºçš„é¢„è®­ç»ƒ"""
        print(f"    åœ¨{protocols}ä¸Šé¢„è®­ç»ƒ {epochs} epochs...")

        data_loader = DataLoader(dataset, batch_size=8, shuffle=True, num_workers=0)
        model.train()
        optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)

        for epoch in range(epochs):
            total_loss = 0.0
            batch_count = 0

            for batch in data_loader:
                try:
                    batch = {k: v.to(self.device) if torch.is_tensor(v) else v
                             for k, v in batch.items()}

                    optimizer.zero_grad()

                    outputs = model(batch['sequence'], protocol=batch['protocol'][0])

                    # ç®€åŒ–çš„æŸå¤±è®¡ç®—
                    boundary_loss = F.cross_entropy(
                        outputs['boundary_logits'].view(-1, 2),
                        batch['boundary_labels'].view(-1)
                    )

                    total_loss_val = boundary_loss
                    total_loss_val.backward()

                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                    optimizer.step()

                    total_loss += total_loss_val.item()
                    batch_count += 1

                except Exception as e:
                    continue

            if batch_count > 0:
                avg_loss = total_loss / batch_count
                print(f"      Epoch {epoch + 1}/{epochs}: Loss={avg_loss:.4f}")

    def _run_improved_simple_experiment(self, source_protocols: List[str],
                                        target_protocol: str, shots: int, episodes: int) -> Dict:
        """è¿è¡Œæ”¹è¿›çš„ç®€å•Few-Shotå®éªŒ"""
        print(f"\nğŸ”¬ æ”¹è¿›çš„ç®€å•Few-Shot {shots}-shot å®éªŒ...")

        episode_results = []

        for episode in range(episodes):
            try:
                # ä¸ºæ¯ä¸ªepisodeåˆ›å»ºç‹¬ç«‹çš„æ¨¡å‹å®ä¾‹
                model = GenericCrossProtocolTransferModel(
                    protocol_names=source_protocols + [target_protocol],
                    d_model=128,
                    encoder_layers=3,
                    num_semantic_types=len(self.data_loader.unified_semantic_types),
                    num_semantic_functions=len(self.data_loader.unified_semantic_functions)
                ).to(self.device)

                # æºåè®®é¢„è®­ç»ƒ
                source_data = []
                for protocol in source_protocols:
                    source_data.extend(self.all_data[protocol][:50])

                if source_data:
                    source_dataset = GenericTransferLearningDataset(
                        source_data,
                        unified_semantic_types=self.data_loader.unified_semantic_types,
                        unified_semantic_functions=self.data_loader.unified_semantic_functions
                    )
                    self._enhanced_pretrain(model, source_dataset, source_protocols, epochs=3)

                # Few-Shoté€‚åº”
                target_data = self.all_data[target_protocol]
                if len(target_data) >= shots + 10:
                    # éšæœºé‡‡æ ·
                    available_data = target_data.copy()
                    random.shuffle(available_data)

                    few_shot_data = available_data[:shots]
                    test_data = available_data[shots:shots + 10]

                    if few_shot_data and test_data:
                        # å¾®è°ƒ
                        few_shot_dataset = GenericTransferLearningDataset(
                            few_shot_data,
                            protocol_filter=target_protocol,
                            unified_semantic_types=self.data_loader.unified_semantic_types,
                            unified_semantic_functions=self.data_loader.unified_semantic_functions
                        )
                        few_shot_loader = DataLoader(few_shot_dataset, batch_size=min(4, shots),
                                                     shuffle=True, num_workers=0)
                        self._enhanced_pretrain(model, few_shot_dataset, [target_protocol], epochs=8)

                        # æµ‹è¯•
                        test_dataset = GenericTransferLearningDataset(
                            test_data,
                            protocol_filter=target_protocol,
                            unified_semantic_types=self.data_loader.unified_semantic_types,
                            unified_semantic_functions=self.data_loader.unified_semantic_functions
                        )

                        result = self._evaluate_simple_model(model, test_dataset, target_protocol)
                        episode_results.append(result)

                if episode % 20 == 0 and episode_results:
                    recent_results = episode_results[-20:]
                    avg_f1 = np.mean([r['overall_f1'] for r in recent_results])
                    print(f"    Episode {episode}/{episodes}: å¹³å‡F1 = {avg_f1:.4f}")

            except Exception as e:
                print(f"    Episode {episode} å¤±è´¥: {e}")
                episode_results.append({
                    'overall_f1': 0.0, 'accuracy': 0.0
                })

            # æ¸…ç†å†…å­˜
            if torch.cuda.is_available():
                torch.cuda.empty_cache()

        if not episode_results:
            return {'success': False, 'error': 'no_valid_episodes'}

        # è®¡ç®—å¹³å‡ç»“æœ
        avg_overall_f1 = np.mean([r['overall_f1'] for r in episode_results])
        avg_accuracy = np.mean([r['accuracy'] for r in episode_results])

        print(f"âœ… æ”¹è¿›ç‰ˆç®€å•Few-Shotç»“æœ:")
        print(f"  æ•´ä½“F1: {avg_overall_f1:.4f}")
        print(f"  å‡†ç¡®ç‡: {avg_accuracy:.4f}")

        return {
            'success': True,
            'avg_overall_f1': avg_overall_f1,
            'avg_accuracy': avg_accuracy,
            'std_overall_f1': np.std([r['overall_f1'] for r in episode_results]),
            'episode_results': episode_results
        }

    def _evaluate_simple_model(self, model, dataset, protocol):
        """è¯„ä¼°ç®€å•æ¨¡å‹"""
        model.eval()
        data_loader = DataLoader(dataset, batch_size=8, shuffle=False, num_workers=0)

        all_predictions = []
        all_labels = []

        with torch.no_grad():
            for batch in data_loader:
                try:
                    batch = {k: v.to(self.device) if torch.is_tensor(v) else v
                             for k, v in batch.items()}

                    outputs = model(batch['sequence'], protocol=protocol)

                    # ç®€åŒ–è¯„ä¼°ï¼šåªçœ‹è¾¹ç•Œæ£€æµ‹
                    boundary_pred = torch.argmax(outputs['boundary_logits'], dim=-1)
                    boundary_labels = batch['boundary_labels']

                    all_predictions.extend(boundary_pred.cpu().numpy().flatten())
                    all_labels.extend(boundary_labels.cpu().numpy().flatten())

                except Exception as e:
                    continue

        if all_predictions and all_labels:
            accuracy = accuracy_score(all_labels, all_predictions)
            f1 = f1_score(all_labels, all_predictions, average='weighted', zero_division=0)
            return {'overall_f1': f1, 'accuracy': accuracy}
        else:
            return {'overall_f1': 0.0, 'accuracy': 0.0}

    def _collate_batch(self, batch_data: List[Dict]) -> Dict:
        """æ•´ç†æ‰¹æ¬¡æ•°æ®"""
        try:
            sequences = torch.stack([item['sequence'] for item in batch_data])
            protocols = [item['protocol'] for item in batch_data]

            return {
                'sequence': sequences,
                'protocol': protocols
            }
        except Exception as e:
            print(f"æ‰¹æ¬¡æ•´ç†å¤±è´¥: {e}")
            batch_size = len(batch_data)
            return {
                'sequence': torch.zeros((batch_size, 256)),
                'protocol': ['unknown'] * batch_size
            }

    def run_comprehensive_study(self, shot_configs: List[int] = [1, 3, 5, 10],
                                methods: List[str] = ['simple', 'prototypical'],
                                episodes: int = 50) -> Dict:
        """è¿è¡Œå…¨é¢çš„Few-Shotå­¦ä¹ ç ”ç©¶"""
        print(f"\nğŸ“Š ä¿®å¤ç‰ˆå…¨é¢Few-Shotå­¦ä¹ ç ”ç©¶")
        print(f"Shoté…ç½®: {shot_configs}")
        print(f"æ–¹æ³•: {methods}")
        print(f"æµ‹è¯•å›åˆ: {episodes}")

        all_results = {}
        available_protocols = list(self.all_data.keys())

        if len(available_protocols) < 2:
            print("âŒ éœ€è¦è‡³å°‘2ä¸ªåè®®è¿›è¡ŒFew-Shotå®éªŒ")
            return {'success': False, 'error': 'insufficient_protocols'}

        # ç”Ÿæˆåè®®å¯¹
        protocol_pairs = []
        for target in available_protocols:
            for source in available_protocols:
                if source != target:
                    protocol_pairs.append(([source], target))

        print(f"åè®®å¯¹æ•°é‡: {len(protocol_pairs)}")

        total_experiments = len(shot_configs) * len(methods) * len(protocol_pairs)
        completed = 0

        for shots in shot_configs:
            all_results[f'{shots}_shot'] = {}

            for method in methods:
                all_results[f'{shots}_shot'][method] = {}

                # ã€é‡è¦ã€‘å¯¹äºprototypicalæ–¹æ³•ï¼Œä½¿ç”¨æ›´ä¸¥æ ¼çš„è®¾ç½®
                current_episodes = episodes // 2 if method == 'prototypical' else episodes

                for source_protocols, target_protocol in protocol_pairs:
                    experiment_key = f"{source_protocols[0]}_to_{target_protocol}"

                    print(f"\n[{completed + 1}/{total_experiments}] {method} {shots}-shot: {experiment_key}")

                    try:
                        result = self.run_few_shot_experiment(
                            source_protocols=source_protocols,
                            target_protocol=target_protocol,
                            shots=shots,
                            episodes=current_episodes,
                            method=method
                        )

                        all_results[f'{shots}_shot'][method][experiment_key] = result

                        if result.get('success', False):
                            main_metric = result.get('avg_overall_f1', 0)
                            print(f"    âœ… æˆåŠŸ: F1 = {main_metric:.4f}")
                        else:
                            print(f"    âŒ å¤±è´¥: {result.get('error', 'unknown')}")

                    except Exception as e:
                        print(f"    âŒ å¼‚å¸¸: {e}")
                        all_results[f'{shots}_shot'][method][experiment_key] = {
                            'success': False, 'error': str(e)
                        }

                    completed += 1

                    # å†…å­˜æ¸…ç†
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()

        # ä¿å­˜ç»“æœ
        self._save_results(all_results)
        return all_results

    def _save_results(self, results: Dict):
        """ä¿å­˜ç»“æœ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"fixed_few_shot_results_{timestamp}.json"

        try:
            def convert_types(obj):
                if isinstance(obj, np.integer):
                    return int(obj)
                elif isinstance(obj, np.floating):
                    return float(obj)
                elif isinstance(obj, np.ndarray):
                    return obj.tolist()
                elif isinstance(obj, dict):
                    return {k: convert_types(v) for k, v in obj.items()}
                elif isinstance(obj, list):
                    return [convert_types(v) for v in obj]
                return obj

            converted_results = convert_types(results)

            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(converted_results, f, indent=2, ensure_ascii=False)

            print(f"\nğŸ’¾ ä¿®å¤ç‰ˆå®éªŒç»“æœå·²ä¿å­˜: {filename}")

        except Exception as e:
            print(f"âŒ ä¿å­˜ç»“æœå¤±è´¥: {e}")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='ä¿®å¤ç‰ˆFew-Shot Learningè·¨åè®®è¿ç§»å®éªŒ')

    parser.add_argument('--data-root', type=str, default='../Msg2', help='æ•°æ®æ ¹ç›®å½•')
    parser.add_argument('--shots', type=int, default=5, help='Few-Shotå­¦ä¹ çš„æ ·æœ¬æ•°')
    parser.add_argument('--episodes', type=int, default=30, help='æµ‹è¯•å›åˆæ•°')
    parser.add_argument('--method', type=str, default='prototypical',
                        choices=['simple', 'prototypical'], help='Few-Shotå­¦ä¹ æ–¹æ³•')
    parser.add_argument('--comprehensive', action='store_true', help='è¿è¡Œå…¨é¢ç ”ç©¶')

    args = parser.parse_args()

    # åˆå§‹åŒ–ä¿®å¤ç‰ˆå®éªŒ
    experiment = FixedFewShotLearningExperiment(args.data_root)

    if args.comprehensive:
        print("ğŸš€ å¯åŠ¨ä¿®å¤ç‰ˆå…¨é¢Few-Shotå­¦ä¹ ç ”ç©¶...")
        results = experiment.run_comprehensive_study(
            shot_configs=[1, 3, 5, 10],
            methods=['simple', 'prototypical'],
            episodes=args.episodes
        )
        print("âœ… ä¿®å¤ç‰ˆFew-Shotå®éªŒå®Œæˆï¼")
    else:
        print(f"ğŸš€ å¯åŠ¨ä¿®å¤ç‰ˆå•ä¸ªFew-Shotå®éªŒ: {args.method} {args.shots}-shot")
        result = experiment.run_few_shot_experiment(
            source_protocols=['modbus'],
            target_protocol='dnp3',
            shots=args.shots,
            episodes=args.episodes,
            method=args.method
        )

        if result.get('success', False):
            print(f"âœ… å®éªŒæˆåŠŸå®Œæˆ! F1 = {result.get('avg_overall_f1', 0):.4f}")
        else:
            print(f"âŒ å®éªŒå¤±è´¥: {result.get('error', 'unknown')}")


if __name__ == "__main__":
    main()