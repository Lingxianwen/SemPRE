#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
SemPRE è¯„ä¼°æ¡†æ¶
åŸºäºçœŸå®æ•°æ®é›†çš„æ€§èƒ½è¯„ä¼°å·¥å…·

è¯„ä¼°æŒ‡æ ‡:
1. Boundary Detection: è¾¹ç•Œæ£€æµ‹å‡†ç¡®ç‡ã€ç²¾ç¡®ç‡ã€å¬å›ç‡ã€F1åˆ†æ•°
2. Field-level Perfection: å­—æ®µçº§å®Œç¾åŒ¹é…ç‡
3. Type Inference: å­—æ®µç±»å‹æ¨æ–­å‡†ç¡®ç‡
4. Semantic Understanding: è¯­ä¹‰ç†è§£å‡†ç¡®ç‡

"""

import os
import sys
import numpy as np
import pandas as pd
import logging
import argparse
from typing import Dict, List, Tuple, Optional, Set
from collections import defaultdict, Counter
from pathlib import Path
import warnings
import json
from dataclasses import dataclass

# å¯¼å…¥ä¸»æ¨¡å—
from advanced_protocol_analyzer import AdvancedProtocolPipeline

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)
warnings.filterwarnings('ignore')


# =============================================================================
# æ•°æ®åŠ è½½å™¨
# =============================================================================

class RealDatasetLoader:
    """çœŸå®æ•°æ®é›†åŠ è½½å™¨"""

    def __init__(self, data_root: str = "Msg2"):
        self.data_root = Path(data_root)
        self.csv_root = self.data_root / "csv"
        self.txt_root = self.data_root / "txt"

        # æ”¯æŒçš„åè®®åˆ—è¡¨
        self.supported_protocols = [
            'smb', 'smb2', 'dns', 's7comm', 'dnp3',
            'modbus', 'ftp', 'tls', 'dhcp'
        ]

    def load_protocol_data(self, protocol_name: str) -> List[Dict]:
        """åŠ è½½åè®®æ•°æ®ä»CSVæ–‡ä»¶"""
        logger.info(f"  åŠ è½½ {protocol_name.upper()} åè®®æ•°æ®...")

        # æ£€æŸ¥CSVæ–‡ä»¶å¤¹
        csv_protocol_dir = self.csv_root / protocol_name
        if not csv_protocol_dir.exists():
            logger.warning(f"  CSVç›®å½•ä¸å­˜åœ¨: {csv_protocol_dir}")
            return []

        # æŸ¥æ‰¾CSVæ–‡ä»¶
        csv_files = list(csv_protocol_dir.glob("*.csv"))
        if not csv_files:
            logger.warning(f"  æ²¡æœ‰æ‰¾åˆ°CSVæ–‡ä»¶: {csv_protocol_dir}")
            return []

        data = []
        for csv_file in csv_files:
            try:
                file_data = self._load_csv_file(csv_file, protocol_name)
                data.extend(file_data)
                logger.info(f"    ä» {csv_file.name} åŠ è½½ {len(file_data)} æ¡æ•°æ®")
            except Exception as e:
                logger.error(f"  åŠ è½½CSVæ–‡ä»¶ {csv_file} å¤±è´¥: {e}")
                continue

        logger.info(f"    æ€»è®¡åŠ è½½ {len(data)} æ¡ {protocol_name.upper()} æ•°æ®")
        return data

    def _load_csv_file(self, csv_file: Path, protocol_name: str) -> List[Dict]:
        """åŠ è½½å•ä¸ªCSVæ–‡ä»¶"""
        data = []

        try:
            # è¯»å–CSVæ–‡ä»¶
            df = pd.read_csv(csv_file)
            logger.info(f"     CSVæ–‡ä»¶ {csv_file.name} åŒ…å« {len(df)} è¡Œæ•°æ®")

            # æ£€æŸ¥å¿…è¦çš„åˆ—
            required_columns = self._get_required_columns(df.columns.tolist())
            if not required_columns:
                logger.warning(f"     CSVæ–‡ä»¶ç¼ºå°‘å¿…è¦åˆ—ï¼Œå°è¯•è‡ªåŠ¨æ¨æ–­...")
                required_columns = self._infer_columns(df.columns.tolist())

            # å¤„ç†æ¯ä¸€è¡Œæ•°æ®
            for index, row in df.iterrows():
                try:
                    sample = self._parse_csv_row(row, index, protocol_name, required_columns)
                    if sample:
                        data.append(sample)
                except Exception as e:
                    logger.debug(f"     è§£æç¬¬ {index} è¡Œå¤±è´¥: {e}")
                    continue

        except Exception as e:
            logger.error(f"  è¯»å–CSVæ–‡ä»¶å¤±è´¥: {e}")

        return data

    def _get_required_columns(self, columns: List[str]) -> Dict[str, str]:
        """è·å–å¿…è¦çš„åˆ—åæ˜ å°„ - ä½¿ç”¨ä¿®å¤åçš„ä¸¤æ­¥åŒ¹é…ç­–ç•¥"""
        column_mapping = {}

        # ç¬¬ä¸€æ­¥ï¼šå°è¯•ç²¾ç¡®åŒ¹é…æœ€å¸¸è§çš„åˆ—å
        for col in columns:
            col_lower = col.lower().strip()
            if col_lower == 'hexdata':
                column_mapping['hex_data'] = col
            if col_lower == 'boundaries':
                column_mapping['boundaries'] = col

        # ç¬¬äºŒæ­¥ï¼šå¦‚æœç²¾ç¡®åŒ¹é…å¤±è´¥ï¼Œå°è¯•æ¨¡å¼åŒ¹é…
        if 'hex_data' not in column_mapping or 'boundaries' not in column_mapping:
            hex_patterns = ['hex_data', 'hex', 'data', 'payload', 'raw_data', 'message', 'packet', 'frame']
            boundary_patterns = ['boundary', 'field_boundaries', 'gt_boundaries', 'ground_truth', 'fields']

            for col in columns:
                col_lower = col.lower().strip()

                # æŸ¥æ‰¾HEXæ•°æ®åˆ—
                if not column_mapping.get('hex_data'):
                    for pattern in hex_patterns:
                        if pattern in col_lower:
                            column_mapping['hex_data'] = col
                            break

                # æŸ¥æ‰¾è¾¹ç•Œæ ‡ç­¾åˆ— - æ’é™¤ä»¥ "has" å¼€å¤´çš„åˆ—
                if not column_mapping.get('boundaries'):
                    if not col_lower.startswith('has'):
                        for pattern in boundary_patterns:
                            if pattern in col_lower:
                                column_mapping['boundaries'] = col
                                break

        return column_mapping

    def _infer_columns(self, columns: List[str]) -> Dict[str, str]:
        """æ¨æ–­åˆ—å"""
        column_mapping = {}

        # å¦‚æœåªæœ‰å°‘æ•°å‡ åˆ—ï¼Œå°è¯•æ¨æ–­
        if len(columns) >= 1:
            column_mapping['hex_data'] = columns[0]  # ç¬¬ä¸€åˆ—é€šå¸¸æ˜¯æ•°æ®

        if len(columns) >= 2:
            column_mapping['boundaries'] = columns[1]  # ç¬¬äºŒåˆ—å¯èƒ½æ˜¯æ ‡ç­¾

        return column_mapping

    def _parse_csv_row(self, row: pd.Series, row_index: int, protocol_name: str, column_mapping: Dict[str, str]) -> Optional[Dict]:
        """è§£æCSVè¡Œæ•°æ®"""
        try:
            # è·å–HEXæ•°æ®
            hex_data = None
            if 'hex_data' in column_mapping:
                hex_data = str(row[column_mapping['hex_data']]).strip()
            else:
                hex_data = str(row.iloc[0]).strip()

            if not hex_data or hex_data.lower() in ['nan', 'none', '']:
                return None

            # æ¸…ç†HEXæ•°æ®
            hex_data = self._clean_hex_data(hex_data)
            if not hex_data:
                return None

            # è½¬æ¢ä¸ºå­—èŠ‚
            try:
                raw_bytes = bytes.fromhex(hex_data)
            except ValueError as e:
                logger.debug(f"     ç¬¬ {row_index} è¡ŒHEXæ•°æ®æ ¼å¼é”™è¯¯: {e}")
                return None

            # è·å–è¾¹ç•Œæ ‡ç­¾
            boundaries = self._parse_boundaries(row, column_mapping, len(raw_bytes), protocol_name)

            # è·å–è¯­ä¹‰ç±»å‹ (å¦‚æœæœ‰)
            semantic_types = self._parse_semantic_types(row, column_mapping)

            # åˆ›å»ºæ ·æœ¬
            sample = {
                'raw_data': hex_data,
                'protocol': protocol_name,
                'bytes': raw_bytes,
                'length': len(raw_bytes),
                'message_type': f'real_{protocol_name}',
                'ground_truth_boundaries': boundaries,
                'semantic_types': semantic_types,
                'source': f'csv_row_{row_index}',
                'row_index': row_index
            }

            return sample

        except Exception as e:
            logger.debug(f"     è§£æç¬¬ {row_index} è¡Œå¤±è´¥: {e}")
            return None

    def _clean_hex_data(self, hex_data: str) -> str:
        """æ¸…ç†HEXæ•°æ®"""
        # ç§»é™¤ç©ºæ ¼ã€å†’å·ã€è¿å­—ç¬¦ç­‰
        hex_data = hex_data.replace(' ', '').replace(':', '').replace('-', '')

        # åªä¿ç•™æœ‰æ•ˆçš„HEXå­—ç¬¦
        hex_data = ''.join(c for c in hex_data if c in '0123456789abcdefABCDEF')

        # ç¡®ä¿é•¿åº¦ä¸ºå¶æ•°
        if len(hex_data) % 2 != 0:
            hex_data = '0' + hex_data

        return hex_data

    def _parse_boundaries(self, row: pd.Series, column_mapping: Dict[str, str],
                          length: int, protocol_name: str) -> List[int]:
        """è§£æè¾¹ç•Œ - ä½¿ç”¨ä¿®å¤åçš„é€»è¾‘"""
        boundaries = [0]  # æ€»æ˜¯åŒ…å«èµ·å§‹ä½ç½®
        csv_boundaries_found = False

        try:
            # 1. å°è¯•ä»æŒ‡å®šåˆ—è·å–è¾¹ç•Œ
            if 'boundaries' in column_mapping:
                boundary_data = str(row[column_mapping['boundaries']]).strip()
                if boundary_data and boundary_data.lower() not in ['nan', 'none', '']:
                    parsed_boundaries = self._parse_boundary_string(boundary_data, length)
                    if parsed_boundaries:
                        boundaries.extend(parsed_boundaries)
                        csv_boundaries_found = True

            # 2. å¦‚æœæ²¡æœ‰æ‰¾åˆ°è¾¹ç•Œæ ‡ç­¾ï¼Œä½¿ç”¨åè®®æ ‡å‡†è¾¹ç•Œ
            if not csv_boundaries_found:
                standard_boundaries = self._get_protocol_standard_boundaries(protocol_name, length)
                boundaries.extend(standard_boundaries)

        except Exception as e:
            logger.debug(f"     è§£æè¾¹ç•Œå¤±è´¥: {e}")
            if not csv_boundaries_found:
                standard_boundaries = self._get_protocol_standard_boundaries(protocol_name, length)
                boundaries.extend(standard_boundaries)

        # ç¡®ä¿åŒ…å«ç»“æŸä½ç½®
        if length not in boundaries:
            boundaries.append(length)

        # å»é‡å¹¶æ’åº
        boundaries = sorted(list(set(boundaries)))

        return boundaries

    def _parse_boundary_string(self, boundary_str: str, length: int) -> List[int]:
        """è§£æè¾¹ç•Œå­—ç¬¦ä¸²"""
        boundaries = []

        try:
            # æ¸…ç†è¾¹ç•Œå­—ç¬¦ä¸²
            boundary_str = boundary_str.strip('[](){}"\'')

            # å°è¯•ä¸åŒçš„åˆ†éš”ç¬¦
            separators = [',', ';', ' ', '|', '\t', '-', '_']

            for sep in separators:
                if sep in boundary_str:
                    parts = boundary_str.split(sep)
                    for part in parts:
                        part = part.strip()
                        try:
                            pos = int(part)
                            if 0 <= pos <= length:
                                boundaries.append(pos)
                        except ValueError:
                            continue
                    break

            # å¦‚æœæ²¡æœ‰åˆ†éš”ç¬¦ï¼Œå°è¯•è§£æå•ä¸ªæ•°å­—
            if not boundaries and boundary_str.isdigit():
                pos = int(boundary_str)
                if 0 <= pos <= length:
                    boundaries.append(pos)

        except Exception as e:
            logger.debug(f"     è§£æè¾¹ç•Œå­—ç¬¦ä¸²å¤±è´¥: {e}")

        return boundaries

    def _parse_semantic_types(self, row: pd.Series, column_mapping: Dict[str, str]) -> Dict[int, str]:
        """è§£æè¯­ä¹‰ç±»å‹ï¼ˆå¦‚æœCSVä¸­åŒ…å«ï¼‰"""
        semantic_types = {}

        try:
            # æŸ¥æ‰¾ SemanticTypes åˆ—
            semantic_col = None
            for col in row.index:
                if 'semantic' in col.lower() and 'type' in col.lower():
                    semantic_col = col
                    break

            if semantic_col:
                semantic_str = str(row[semantic_col])
                if semantic_str and semantic_str.lower() not in ['nan', 'none', '']:
                    # å°è¯•è§£æJSONæ ¼å¼
                    try:
                        import json
                        semantic_types = json.loads(semantic_str.replace("'", '"'))
                        # è½¬æ¢é”®ä¸ºæ•´æ•°
                        semantic_types = {int(k): v for k, v in semantic_types.items()}
                    except:
                        pass

        except Exception as e:
            logger.debug(f"     è§£æè¯­ä¹‰ç±»å‹å¤±è´¥: {e}")

        return semantic_types

    def _get_protocol_standard_boundaries(self, protocol_name: str, length: int) -> List[int]:
        """è·å–åè®®æ ‡å‡†è¾¹ç•Œ"""
        boundaries = []

        protocol_specs = {
            'dns': [2, 4, 6, 8, 10, 12],
            'modbus': [2, 4, 6, 7, 8],
            'smb': [4, 5, 6, 8, 32],
            'smb2': [4, 6, 8, 12, 16, 20, 24],
            'dhcp': [1, 2, 3, 4, 8, 12, 16, 20, 24, 28],
            'dnp3': [2, 3, 4, 6, 8, 10],
            'ftp': [2, 4],
            'tls': [1, 3, 5, 6, 9],
            's7comm': [2, 4, 6, 8, 10, 12]
        }

        if protocol_name in protocol_specs:
            boundaries = [pos for pos in protocol_specs[protocol_name] if pos < length]

        return boundaries

    def get_available_protocols(self) -> List[str]:
        """è·å–å¯ç”¨çš„åè®®åˆ—è¡¨"""
        available = []

        if self.csv_root.exists():
            for protocol_dir in self.csv_root.iterdir():
                if protocol_dir.is_dir() and protocol_dir.name in self.supported_protocols:
                    csv_files = list(protocol_dir.glob("*.csv"))
                    if csv_files:
                        available.append(protocol_dir.name)

        return available


# =============================================================================
# SemPREè¯„ä¼°å™¨
# =============================================================================

class SemPREEvaluator:
    """SemPREæ€§èƒ½è¯„ä¼°å™¨"""

    def __init__(self):
        self.debug_mode = False

    def evaluate_boundaries(self, predicted_boundaries: List[int],
                            ground_truth_boundaries: List[int],
                            sequence_length: int,
                            debug_info: str = "") -> Dict[str, float]:
        """è¯„ä¼°è¾¹ç•Œæ£€æµ‹æ€§èƒ½"""

        if self.debug_mode:
            logger.debug(f"è¯„ä¼°è¾¹ç•Œ {debug_info}")
            logger.debug(f"  é¢„æµ‹è¾¹ç•Œ: {predicted_boundaries}")
            logger.debug(f"  çœŸå®è¾¹ç•Œ: {ground_truth_boundaries}")
            logger.debug(f"  åºåˆ—é•¿åº¦: {sequence_length}")

        return self._precision_evaluation(predicted_boundaries, ground_truth_boundaries, sequence_length)

    def _precision_evaluation(self, predicted_boundaries: List[int],
                              ground_truth_boundaries: List[int],
                              sequence_length: int) -> Dict[str, float]:
        """ç²¾ç¡®è¯„ä¼°ç®—æ³•"""

        # ç¡®ä¿è¾¹ç•Œåˆ—è¡¨åŒ…å«èµ·å§‹å’Œç»“æŸä½ç½®
        pred_boundaries = sorted(list(set(predicted_boundaries + [0, sequence_length])))
        true_boundaries = sorted(list(set(ground_truth_boundaries + [0, sequence_length])))

        # ç§»é™¤è¶…å‡ºèŒƒå›´çš„è¾¹ç•Œ
        pred_boundaries = [b for b in pred_boundaries if 0 <= b <= sequence_length]
        true_boundaries = [b for b in true_boundaries if 0 <= b <= sequence_length]

        if self.debug_mode:
            logger.debug(f"  æ ‡å‡†åŒ–é¢„æµ‹è¾¹ç•Œ: {pred_boundaries}")
            logger.debug(f"  æ ‡å‡†åŒ–çœŸå®è¾¹ç•Œ: {true_boundaries}")

        # 1. è¾¹ç•Œå‡†ç¡®ç‡ï¼ˆé€ä½ç½®æ¯”è¾ƒï¼‰
        accuracy = self._calculate_position_accuracy(pred_boundaries, true_boundaries, sequence_length)

        # 2. è¾¹ç•Œç²¾ç¡®ç‡å’Œå¬å›ç‡
        precision, recall = self._calculate_boundary_precision_recall(pred_boundaries, true_boundaries)

        # 3. F1åˆ†æ•°
        f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0

        # 4. å­—æ®µçº§å®Œç¾åŒ¹é…ç‡
        perfection = self._calculate_field_perfection(pred_boundaries, true_boundaries, sequence_length)

        if self.debug_mode:
            logger.debug(f"  å‡†ç¡®ç‡: {accuracy:.4f}")
            logger.debug(f"  ç²¾ç¡®ç‡: {precision:.4f}")
            logger.debug(f"  å¬å›ç‡: {recall:.4f}")
            logger.debug(f"  F1åˆ†æ•°: {f1_score:.4f}")
            logger.debug(f"  å®Œç¾ç‡: {perfection:.4f}")

        return {
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1_score': f1_score,
            'perfection': perfection
        }

    def _calculate_position_accuracy(self, pred_boundaries: List[int],
                                     true_boundaries: List[int],
                                     sequence_length: int) -> float:
        """è®¡ç®—ä½ç½®çº§å‡†ç¡®ç‡"""
        if sequence_length == 0:
            return 1.0

        pred_set = set(pred_boundaries)
        true_set = set(true_boundaries)

        correct_positions = 0
        for pos in range(sequence_length + 1):
            pred_is_boundary = pos in pred_set
            true_is_boundary = pos in true_set
            if pred_is_boundary == true_is_boundary:
                correct_positions += 1

        return correct_positions / (sequence_length + 1)

    def _calculate_boundary_precision_recall(self, pred_boundaries: List[int],
                                             true_boundaries: List[int]) -> Tuple[float, float]:
        """è®¡ç®—è¾¹ç•Œçº§ç²¾ç¡®ç‡å’Œå¬å›ç‡"""
        pred_set = set(pred_boundaries)
        true_set = set(true_boundaries)

        # ç²¾ç¡®ç‡: é¢„æµ‹çš„è¾¹ç•Œä¸­æœ‰å¤šå°‘æ˜¯æ­£ç¡®çš„
        if len(pred_boundaries) > 0:
            true_positives = len(pred_set & true_set)
            precision = true_positives / len(pred_boundaries)
        else:
            precision = 0.0

        # å¬å›ç‡: çœŸå®è¾¹ç•Œä¸­æœ‰å¤šå°‘è¢«é¢„æµ‹åˆ°
        if len(true_boundaries) > 0:
            true_positives = len(pred_set & true_set)
            recall = true_positives / len(true_boundaries)
        else:
            recall = 1.0 if len(pred_boundaries) == 0 else 0.0

        return precision, recall

    def _calculate_field_perfection(self, pred_boundaries: List[int],
                                    true_boundaries: List[int],
                                    sequence_length: int) -> float:
        """è®¡ç®—å­—æ®µçº§å®Œç¾åŒ¹é…ç‡"""

        # å°†è¾¹ç•Œè½¬æ¢ä¸ºå­—æ®µèŒƒå›´
        pred_fields = self._boundaries_to_fields(pred_boundaries, sequence_length)
        true_fields = self._boundaries_to_fields(true_boundaries, sequence_length)

        if self.debug_mode:
            logger.debug(f"  é¢„æµ‹å­—æ®µ: {pred_fields}")
            logger.debug(f"  çœŸå®å­—æ®µ: {true_fields}")

        if not true_fields:
            return 1.0 if not pred_fields else 0.0

        # è®¡ç®—å®Œå…¨åŒ¹é…çš„å­—æ®µæ•°
        pred_fields_set = set(pred_fields)
        true_fields_set = set(true_fields)

        perfect_matches = len(pred_fields_set & true_fields_set)
        total_true_fields = len(true_fields_set)

        perfection = perfect_matches / total_true_fields if total_true_fields > 0 else 0.0

        if self.debug_mode:
            logger.debug(f"  å®Œç¾åŒ¹é…å­—æ®µæ•°: {perfect_matches}")
            logger.debug(f"  æ€»çœŸå®å­—æ®µæ•°: {total_true_fields}")

        return perfection

    def _boundaries_to_fields(self, boundaries: List[int], length: int) -> List[Tuple[int, int]]:
        """å°†è¾¹ç•Œè½¬æ¢ä¸ºå­—æ®µèŒƒå›´"""
        if not boundaries:
            return [(0, length)] if length > 0 else []

        fields = []
        boundaries = sorted(list(set(boundaries)))

        # ç¡®ä¿åŒ…å«èµ·å§‹å’Œç»“æŸè¾¹ç•Œ
        if 0 not in boundaries:
            boundaries.insert(0, 0)
        if length not in boundaries:
            boundaries.append(length)

        # ç”Ÿæˆå­—æ®µèŒƒå›´
        for i in range(len(boundaries) - 1):
            start = boundaries[i]
            end = boundaries[i + 1]

            if start < end and start < length:
                fields.append((start, min(end, length)))

        return fields


# =============================================================================
#SemPREå®éªŒç®¡ç†å™¨
# =============================================================================

class SemPREExperiment:
    """SemPREå®éªŒç®¡ç†å™¨"""

    def __init__(self, data_root: str = "Msg2"):
        self.data_loader = RealDatasetLoader(data_root)
        self.analyzer = AdvancedProtocolPipeline()
        self.evaluator = SemPREEvaluator()
        self.results = {}
        self.debug_mode = False

    def analyze_messages(self, messages: List[bytes]) -> Dict:
        """é€‚é…å±‚ï¼šå°†æ¶ˆæ¯åˆ—è¡¨è½¬æ¢ä¸ºSemPREå¯åˆ†æçš„æ ¼å¼"""
        # ç›´æ¥è°ƒç”¨å„ä¸ªç»„ä»¶è¿›è¡Œåˆ†æ
        try:
            # æ­¥éª¤2: åè®®é¢„åˆ†æ
            pre_analysis = self.analyzer.pre_analyzer.analyze(messages)
            
            # æ­¥éª¤3: å­—æ®µæ£€æµ‹
            from advanced_protocol_analyzer import ContextAwareFieldDetector
            field_detector = ContextAwareFieldDetector(pre_analysis, self.analyzer.logger)
            field_detection = field_detector.detect(messages)
            
            # æ„å»ºåˆ†æç»“æœ
            analysis_result = {
                'step2_pre_analysis': {
                    'protocol': pre_analysis['protocol_signature'].protocol_type,
                    'confidence': pre_analysis['protocol_signature'].confidence,
                    'variant': pre_analysis['protocol_signature'].variant,
                    'function_count': len(pre_analysis['function_profiles']),
                },
                'step3_field_detection': {
                    'field_candidates': field_detection['field_candidates'],
                    'length_fields': len(field_detection['length_fields']),
                    'offset_fields': len(field_detection['offset_fields']),
                    'dependency_edges': len(field_detection.get('dependency_graph', [])),
                    'total_fields': len(field_detection['field_candidates'])
                }
            }
            
            return analysis_result
            
        except Exception as e:
            logger.error(f"åˆ†æå¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return {
                'step2_pre_analysis': {},
                'step3_field_detection': {'field_candidates': []}
            }


    def enable_debug(self):
        """å¯ç”¨è°ƒè¯•æ¨¡å¼"""
        self.debug_mode = True
        self.evaluator.debug_mode = True

    def run_experiments(self, protocols: List[str] = None, sample_limit: int = None):
        """è¿è¡Œå®éªŒ"""
        # è·å–å¯ç”¨åè®®
        available_protocols = self.data_loader.get_available_protocols()

        if protocols is None:
            protocols = available_protocols
        else:
            protocols = [p for p in protocols if p in available_protocols]

        if not protocols:
            logger.error("  æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„åè®®æ•°æ®")
            return

        logger.info("ğŸš€ SemPREå®éªŒå¼€å§‹")
        logger.info(f"ğŸ“‚ æ•°æ®ç›®å½•: {self.data_loader.data_root}")
        logger.info(f" æµ‹è¯•åè®®: {protocols}")
        if sample_limit:
            logger.info(f"  æ ·æœ¬é™åˆ¶: {sample_limit}")
        logger.info("=" * 70)

        for protocol in protocols:
            logger.info(f"\n  æµ‹è¯•åè®®: {protocol.upper()}")
            logger.info("-" * 50)

            # åŠ è½½çœŸå®æ•°æ®
            data = self.data_loader.load_protocol_data(protocol)

            if not data:
                logger.warning(f"  è·³è¿‡ {protocol}: æ— æ•°æ®")
                continue

            # é™åˆ¶æ ·æœ¬æ•°é‡ï¼ˆç”¨äºè°ƒè¯•ï¼‰
            if sample_limit and len(data) > sample_limit:
                data = data[:sample_limit]
                logger.info(f"    é™åˆ¶æ ·æœ¬æ•°é‡ä¸º: {sample_limit}")

            # å‡†å¤‡æ¶ˆæ¯åˆ—è¡¨
            messages = [sample['bytes'] for sample in data]

            try:
                # è¿è¡ŒSemPREç®—æ³•
                logger.info(f"   ğŸ” è¿è¡ŒSemPREç®—æ³•...")
                analysis_result = self.analyze_messages(messages)

                # æå–é¢„æµ‹çš„è¾¹ç•Œ
                predicted_boundaries_list = self._extract_boundaries_from_analysis(
                    analysis_result, messages, protocol
                )

                # è¯„ä¼°æ€§èƒ½
                logger.info(f"    è¯„ä¼°æ€§èƒ½...")
                all_metrics = []

                for i, (sample, pred_boundaries) in enumerate(zip(data, predicted_boundaries_list)):
                    true_boundaries = sample['ground_truth_boundaries']
                    length = sample['length']

                    debug_info = f"{protocol}_{i}" if self.debug_mode else ""
                    metrics = self.evaluator.evaluate_boundaries(
                        pred_boundaries, true_boundaries, length, debug_info
                    )
                    all_metrics.append(metrics)

                    # è°ƒè¯•æ¨¡å¼ä¸‹æ˜¾ç¤ºå‰å‡ ä¸ªæ ·æœ¬çš„è¯¦ç»†ä¿¡æ¯
                    if self.debug_mode and i < 3:
                        logger.info(f"   ğŸ” æ ·æœ¬ {i}: é¢„æµ‹è¾¹ç•Œ={pred_boundaries}, çœŸå®è¾¹ç•Œ={true_boundaries}")
                        logger.info(f"        å®Œç¾ç‡={metrics['perfection']:.4f}")

                # è®¡ç®—å¹³å‡æŒ‡æ ‡
                avg_metrics = {}
                for key in ['accuracy', 'precision', 'recall', 'f1_score', 'perfection']:
                    values = [m[key] for m in all_metrics if not np.isnan(m[key])]
                    avg_metrics[key] = np.mean(values) if values else 0.0

                # ä¿å­˜ç»“æœ
                self.results[protocol] = {
                    'sample_count': len(data),
                    'metrics': avg_metrics,
                    'csv_rows': len(data),
                    'individual_metrics': all_metrics,
                    'analysis_result': self._serialize_analysis_result(analysis_result)
                }

                # æ˜¾ç¤ºç»“æœ
                logger.info(f"    ç»“æœ:")
                logger.info(f"      CSVè¡Œæ•°: {len(data)}")
                logger.info(f"      æ ·æœ¬æ•°é‡: {len(data)}")
                logger.info(f"      å‡†ç¡®ç‡: {avg_metrics['accuracy']:.4f}")
                logger.info(f"      ç²¾ç¡®ç‡: {avg_metrics['precision']:.4f}")
                logger.info(f"      å¬å›ç‡: {avg_metrics['recall']:.4f}")
                logger.info(f"      F1åˆ†æ•°: {avg_metrics['f1_score']:.4f}")
                logger.info(f"      å­—æ®µIoU: {avg_metrics['perfection']:.4f}")

                # åˆ†æå®Œç¾åŒ¹é…åˆ†å¸ƒ (100%è¾¹ç•Œæ­£ç¡®)
                perfection_values = [m['perfection'] for m in all_metrics]
                perfect_count = sum(1 for p in perfection_values if p >= 0.9999)
                logger.info(f"      å®Œç¾åŒ¹é…æ ·æœ¬: {perfect_count}/{len(data)} ({perfect_count / len(data) * 100:.1f}%)")

            except Exception as e:
                logger.error(f"  å¤„ç† {protocol} æ—¶å‡ºé”™: {e}")
                import traceback
                logger.error(traceback.format_exc())
                self.results[protocol] = {
                    'sample_count': 0,
                    'csv_rows': 0,
                    'metrics': {'accuracy': 0, 'precision': 0, 'recall': 0,
                                'f1_score': 0, 'perfection': 0},
                    'error': str(e)
                }

    def _extract_boundaries_from_analysis(self, analysis_result: Dict,
                                          messages: List[bytes],
                                          protocol: str) -> List[List[int]]:
        """ä»SemPREåˆ†æç»“æœä¸­æå–è¾¹ç•Œ"""
        boundaries_list = []

        try:
            # ä»step3çš„å­—æ®µæ£€æµ‹ç»“æœä¸­æå–è¾¹ç•Œ
            step3 = analysis_result.get('step3_field_detection', {})
            field_candidates = step3.get('field_candidates', [])

            # ä¸ºæ¯ä¸ªæ¶ˆæ¯æå–è¾¹ç•Œ
            for msg_idx, msg in enumerate(messages):
                msg_length = len(msg)
                boundaries = [0]  # èµ·å§‹ä½ç½®

                # æå–è¯¥æ¶ˆæ¯çš„å­—æ®µè¾¹ç•Œ
                for field in field_candidates:
                    if hasattr(field, 'start') and hasattr(field, 'end'):
                        # ç¡®ä¿å­—æ®µè¾¹ç•Œåœ¨æ¶ˆæ¯èŒƒå›´å†…
                        if field.start < msg_length:
                            boundaries.append(field.start)
                        if field.end < msg_length:
                            boundaries.append(field.end)

                # æ·»åŠ ç»“æŸä½ç½®
                boundaries.append(msg_length)

                # å»é‡å¹¶æ’åº
                boundaries = sorted(list(set(boundaries)))
                boundaries_list.append(boundaries)

        except Exception as e:
            logger.warning(f"     æå–è¾¹ç•Œå¤±è´¥: {e}")
            # å¦‚æœæå–å¤±è´¥ï¼Œä¸ºæ¯ä¸ªæ¶ˆæ¯è¿”å›é»˜è®¤è¾¹ç•Œ
            for msg in messages:
                boundaries_list.append([0, len(msg)])

        return boundaries_list

    def _serialize_analysis_result(self, analysis_result: Dict) -> Dict:
        """åºåˆ—åŒ–åˆ†æç»“æœä»¥ä¾¿ä¿å­˜"""
        try:
            serialized = {}

            # æå–å…³é”®ç»Ÿè®¡ä¿¡æ¯
            if 'step2_pre_analysis' in analysis_result:
                s2 = analysis_result['step2_pre_analysis']
                serialized['protocol'] = s2.get('protocol', 'unknown')
                serialized['confidence'] = s2.get('confidence', 0.0)
                serialized['function_count'] = s2.get('function_count', 0)

            if 'step3_field_detection' in analysis_result:
                s3 = analysis_result['step3_field_detection']
                serialized['field_count'] = s3.get('total_fields', 0)
                serialized['length_fields'] = s3.get('length_fields', 0)
                serialized['offset_fields'] = s3.get('offset_fields', 0)
                serialized['dependency_edges'] = s3.get('dependency_edges', 0)

            # SemPREåˆ›æ–°åŠŸèƒ½ç»Ÿè®¡
            if 'step3_5_semantic_graph' in analysis_result:
                s3_5 = analysis_result['step3_5_semantic_graph']
                serialized['semantic_nodes'] = s3_5.get('total_nodes', 0)
                serialized['semantic_edges'] = s3_5.get('total_edges', 0)
                serialized['cross_message_deps'] = s3_5.get('cross_message_dependencies', 0)

            if 'step4_5_unknown_function_inference' in analysis_result:
                s4_5 = analysis_result['step4_5_unknown_function_inference']
                serialized['inferred_functions'] = s4_5.get('inferred_count', 0)

            return serialized

        except Exception as e:
            logger.warning(f"     åºåˆ—åŒ–åˆ†æç»“æœå¤±è´¥: {e}")
            return {}

    def generate_detailed_report(self):
        """ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š"""
        logger.info(f"\n" + "=" * 70)
        logger.info(" SemPREå®éªŒè¯¦ç»†æŠ¥å‘Š")
        logger.info("=" * 70)

        if not self.results:
            logger.warning("  æ²¡æœ‰å®éªŒç»“æœ")
            return

        # åˆ›å»ºç»“æœè¡¨æ ¼
        report_data = []
        for protocol, result in self.results.items():
            metrics = result['metrics']
            report_data.append({
                'Protocol': protocol.upper(),
                'CSV_Rows': result.get('csv_rows', 0),
                'Samples': result['sample_count'],
                'Accuracy': f"{metrics['accuracy']:.4f}",
                'Precision': f"{metrics['precision']:.4f}",
                'Recall': f"{metrics['recall']:.4f}",
                'F1-score': f"{metrics['f1_score']:.4f}",
                'Perfection': f"{metrics['perfection']:.4f}"
            })

        # æ˜¾ç¤ºè¡¨æ ¼
        df = pd.DataFrame(report_data)
        print("\nSemPREå®éªŒç»“æœè¡¨æ ¼:")
        print(df.to_string(index=False))

        # è®¡ç®—æ€§èƒ½ç»Ÿè®¡
        logger.info(f"\n æ€§èƒ½ç»Ÿè®¡:")
        valid_results = [r for r in self.results.values() if 'error' not in r]

        if valid_results:
            total_samples = sum(r['sample_count'] for r in valid_results)
            total_csv_rows = sum(r.get('csv_rows', 0) for r in valid_results)
            avg_field_iou = np.mean([r['metrics']['perfection'] for r in valid_results])
            avg_f1 = np.mean([r['metrics']['f1_score'] for r in valid_results])
            avg_accuracy = np.mean([r['metrics']['accuracy'] for r in valid_results])

            logger.info(f"   æ€»CSVè¡Œæ•°: {total_csv_rows}")
            logger.info(f"   æ€»æ ·æœ¬æ•°: {total_samples}")
            logger.info(f"   å¹³å‡å‡†ç¡®ç‡: {avg_accuracy:.4f}")
            logger.info(f"   å¹³å‡F1åˆ†æ•°: {avg_f1:.4f}")
            logger.info(f"   å¹³å‡å­—æ®µIoU: {avg_field_iou:.4f}")

            # åˆ†æå®Œç¾åŒ¹é…åˆ†å¸ƒ (100%è¾¹ç•Œæ­£ç¡®çš„æ ·æœ¬)
            logger.info(f"\n å®Œç¾åŒ¹é…åˆ†æ:")
            for protocol, result in self.results.items():
                if 'error' not in result and 'individual_metrics' in result:
                    individual_perfections = [m['perfection'] for m in result['individual_metrics']]
                    perfect_count = sum(1 for p in individual_perfections if p >= 0.9999)
                    total_count = len(individual_perfections)
                    logger.info(
                        f"   {protocol.upper()}: {perfect_count}/{total_count} ({perfect_count / total_count * 100:.1f}%) å®Œç¾åŒ¹é…")

            # SemPREåˆ›æ–°åŠŸèƒ½ç»Ÿè®¡
            logger.info(f"\n SemPREåˆ›æ–°åŠŸèƒ½ç»Ÿè®¡:")
            for protocol, result in self.results.items():
                if 'error' not in result and 'analysis_result' in result:
                    ar = result['analysis_result']
                    if ar.get('semantic_nodes', 0) > 0:
                        logger.info(f"   {protocol.upper()}:")
                        logger.info(f"      - è¯­ä¹‰èŠ‚ç‚¹: {ar.get('semantic_nodes', 0)}")
                        logger.info(f"      - è·¨æ¶ˆæ¯ä¾èµ–: {ar.get('cross_message_deps', 0)}")
                        logger.info(f"      - æ¨æ–­åŠŸèƒ½ç : {ar.get('inferred_functions', 0)}")

        else:
            logger.warning("   æ²¡æœ‰æœ‰æ•ˆçš„å®éªŒç»“æœ")

        logger.info(f"\n SemPREç‰¹è‰²:")
        logger.info("   1. å¤šç²’åº¦è¯­ä¹‰æ„ŸçŸ¥å›¾ - ç†è§£å­—æ®µä¹‹é—´çš„è¯­ä¹‰å…³ç³»")
        logger.info("   2. è·¨æ¶ˆæ¯ä¾èµ–æŒ–æ˜ - Request-Responseé…å¯¹åˆ†æ")
        logger.info("   3. æœªçŸ¥åŠŸèƒ½ç é›¶æ ·æœ¬æ¨æ–­ - è‡ªåŠ¨æ¨æ–­æœªçŸ¥åŠŸèƒ½ç±»å‹")
        logger.info("   4. Bitçº§è¯­ä¹‰æ³¨æ„åŠ› - ç²¾ç»†åˆ°ä½çº§åˆ«çš„åˆ†æ")
        logger.info("   5. ç®—æœ¯/é€»è¾‘çº¦æŸå‘ç° - å­—æ®µé—´æ•°å­¦å…³ç³»è¯†åˆ«")

    def save_results(self, output_file: str):
        """ä¿å­˜ç»“æœåˆ°JSONæ–‡ä»¶"""
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(self.results, f, indent=2, ensure_ascii=False)
            logger.info(f"\n ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        except Exception as e:
            logger.error(f"  ä¿å­˜ç»“æœå¤±è´¥: {e}")


# =============================================================================
# ä¸»å‡½æ•°
# =============================================================================

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='SemPREçœŸå®æ•°æ®é›†å®éªŒè¯„ä¼°')

    parser.add_argument('--data-root', default='Msg2',
                        help='æ•°æ®é›†æ ¹ç›®å½• (é»˜è®¤: Msg2)')

    parser.add_argument('--protocols', nargs='+',
                        choices=['smb', 'smb2', 'dns', 's7comm', 'dnp3',
                                 'modbus', 'ftp', 'tls', 'dhcp'],
                        help='è¦æµ‹è¯•çš„åè®®åˆ—è¡¨')

    parser.add_argument('--debug', action='store_true',
                        help='å¯ç”¨è°ƒè¯•æ¨¡å¼')

    parser.add_argument('--sample-limit', type=int,
                        help='é™åˆ¶æ¯ä¸ªåè®®çš„æ ·æœ¬æ•°é‡ï¼ˆç”¨äºè°ƒè¯•ï¼‰')

    parser.add_argument('--output', default='./output/Sempre_results.json',
                        help='è¾“å‡ºç»“æœæ–‡ä»¶è·¯å¾„')

    args = parser.parse_args()

    # åˆ›å»ºå®éªŒç®¡ç†å™¨
    experiment = SemPREExperiment(args.data_root)

    if args.debug:
        experiment.enable_debug()
        logger.info("ğŸ”§ è°ƒè¯•æ¨¡å¼å·²å¯ç”¨")

    logger.info(f"ğŸŒŸ SemPREå®éªŒè®¾ç½®:")
    logger.info(f"   æ•°æ®æ ¹ç›®å½•: {args.data_root}")
    logger.info(f"   æµ‹è¯•åè®®: {args.protocols or 'ALL'}")
    logger.info(f"   è°ƒè¯•æ¨¡å¼: {args.debug}")
    if args.sample_limit:
        logger.info(f"   æ ·æœ¬é™åˆ¶: {args.sample_limit}")

    # è¿è¡Œå®éªŒ
    experiment.run_experiments(protocols=args.protocols, sample_limit=args.sample_limit)

    # ç”ŸæˆæŠ¥å‘Š
    experiment.generate_detailed_report()

    # ä¿å­˜ç»“æœ
    experiment.save_results(args.output)

    logger.info("\n SemPREå®éªŒå®Œæˆï¼")


if __name__ == "__main__":
    main()
